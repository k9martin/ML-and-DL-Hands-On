{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hands On: Introduction to Artificial Neural Networks with Keras.</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales artificiales son el elemento base del Deep Learning. Son estructuras muy versátiles, potentes y escalables que permiten realizar tareas complejas de ML. En este notebook vamos a ver una introducción a ellas:\n",
    "\n",
    "  - Neuronas biológicas. Computación lógica. \n",
    "  - El perceptrón. Perceptrón multicapa (MLP) y backpropagation.\n",
    "  - MLP de regresión y de clasificación.\n",
    "  - Implementación de MLP con Keras. Construcción de distintos modelos. Guardado. Uso de callbacks y visualización.\n",
    "  - Fine-Tuning de hiperparámetros. Número de capas ocultas, de neuronas por capa, learning rate y otros.\n",
    "  \n",
    "En este notebook se verá una introducción a las redes neuronales artificiales, en concreto los Multi-Layer Perceptrons, y su implementación básica desde Keras. También se estudiarán algunas aplicaciones de las mismas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version de tf: 1.13.0-dev20190224\n",
      "version de keras: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# Imports generales:\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import urllib\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Algoritmos:\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Para esconder los warning:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Comprobar la versión de tensorflow:\n",
    "print('version de tf:',tf.__version__)\n",
    "print('version de keras:',keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neuronas Biológicas. Computación lógica. </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera descripción de redes neuronales artificiales (ANNs) data de 1943, de un paper de Warren McCulloch y Walter Pitts. Introducía el concepto de un modelo computacional capaz de hacer cálculos complejos por medio de lógica proposicional. Tras el boom inicial en los 60s, en los que se pensaba que se iban a alcanzar máquinas lo suficientemente capaces de mantener una conversación fluida, se diluyó un poco el interés por estas estructuras lógicas. Este interés se ha visto revivido ahora por una serie de factores:\n",
    "\n",
    "    - La cantidad de datos disponibles hoy es gigante en comparación a la que existía en otras épocas.\n",
    "    - La capacidad computacional ha aumentado exponencialmente en las últimas décadas.\n",
    "    - Los algoritmos han sido perfeccionados. A pesar de ser sólo pequeños cambios, tienen un fuerte impacto en el rendimiento.\n",
    "    - Algunas limitaciones teóricas de las ANN han resultado ser inofensivas. Por ejemplo, se consideraba que las ANN tendrían tendencia a converger a mínimos locales, pero en la práctica no sucede casi nunca. De hecho, cuando sucede, suelen ser mínimos locales muy cercanos al global.\n",
    "    - Existe un círculo vicioso con las ANN: a medida que se hacen grandes avances, estos son tan publicitados que despiertan nuevo interés, volviendo al foco de atención y a protagonizar avances.\n",
    "\n",
    "Pero antes de introducirnos por completo en las ANN, vamos a ver cómo son las neuronas biológicas que inspiraron la idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neuronas biológicas.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las neuronas biológicas son un tipo especial de célula. Está compuesto de un cuerpo (con el núcleo y los componentes complejos) y brazos llamados dendritas, además de una extensión larga llamada axón. El axón tiene unas ramas llamadas telodendritas, que terminan en unas estructuras minúsculas llamadas terminales sinápticas (o solo sinapsis), conectadas a las dendritas o el cuerpo de otras neuronas. Las neuronas biológicas se mandan impulsos eléctricos llamadas señales, y cuando una neurona recibe muchas señales empieza a emitir las suyas propias.\n",
    "\n",
    "Las neuronas se conectan en redes de billones de las mismas. Estas forman las Biological Neural Networks (BNN), que sirvieron de inspiración a las ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computación lógica con neuronas</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo propuesto por McCulloch y Pitts de neurona artificial se compone de uno o más inputs binarios (on/off) y un output binario. La neurona artificial se activa cuando un cierto número de inputs están activos. Con esta neurona como ladrillo base, se pueden construir redes de neuronas artificiales que comuten cualquier proposición lógica que se quiera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>El Perceptrón</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptrón es una de las arquitecturas más simples de ANN. Está basado en una unidad de neurona llamada \"threshold logic unit\"(TLU) o \"linear threshold unit\"(LTU): los inputs y outputs son números en lugar de valores binarios con un peso asociado, y la neurona se activa cuando la suma de estos supera un cierto threshold (utilizando una función escalón, típicamente la heaviside).\n",
    "\n",
    "Un sólo TLU se puede usar para clasificación binaria (como si fuese un clasificador de regresión logítica o de SVM lineal). Un perceptrón es una composición de TLUs de una capa, donde cada TLU se conecta a todos los inputs. Cuando cada neurona de una capa se conecta a todas las de la anterior capa se llama \"fully connected layer\" o \"dense layer\". Además, en la primera capa y a la hora de representar, se añade un neuron bias = 1.\n",
    "\n",
    "A la hora de computar todos los outputs de una capa de neuronas artificiales para varias instancias a la vez, basta con representar el sistema de forma matricial:\n",
    "\n",
    "    h_(w,b) = phi(XW + b)\n",
    "    \n",
    "Donde X es la matriz de inputs (una fila por instancia, una columna por variable), la matriz W contiene los pesos del sistema excepto el término bias (una fila por neurona input y una columna por neurona en la capa), el vector b contiene los pesos entre el término bias y las neuronas artificiales y la función phi es la función de activación (en TLUs es una función escalón).\n",
    "\n",
    "Pero, ¿cómo se entrena un perceptrón?. Para ello se utiliza un algoritmo inspirado en la Hebb's rule: \"Cells that fire together, wire together\". El concepto de entrenamiento es que el peso entre dos neuronas se incrementa si comparten el mismo output, pero además se refuerzan aquellas conexiones que reducen el error. \n",
    "\n",
    "Más específicamente, el perceptrón se entrena pasando las instances de 1 en 1, y para cada instance se predice. Si existe un error en la predicción se refuerzan los pesos de aquellas conexiones que habrían contribuido a predecir bien:\n",
    "\n",
    "    w_(i,j) = w_(i,j) + nu * (y_j - y^_j)*x_i\n",
    "    \n",
    "Donde w_(i,j) es el peso entre i y j, x_i es el valor input de la instance de entrenamiento, y^_j es el output de la neurona para el entrenamiento, y_j el output correcto y nu el learning rate.\n",
    "\n",
    "La decision boundary de cada neurona es lineal, por lo que los perceptrones no pueden captar patrones complejos. Sin embargo si las instances son linealmente separables el algoritmo siempre converge a una solución (\"Perceptron convergence theorem\").\n",
    "\n",
    "Vamos a ver un ejemplo de perceptrón con la clase que proporciona Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El predict del perceptrón : [1]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "print('El predict del perceptrón :',per_clf.predict([[2, 0.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo del perceptrón se parece bastante al del Stochastic Gradient Descent. De hecho un Perceptron de Sklearn es similar a usar un SGDClassifier con los hiperparámetros loss='perceptron', learning_rate='constant', eta0=1 y penalty = None. \n",
    "\n",
    "Al contrario que las logistic regression, el Perceptron no devuelve probabilidades de clases porque está basado en un hard threshold (por eso es mejor usar Logistic Regression que Perceptron).\n",
    "\n",
    "Existen ciertas limitaciones a los perceptrones (mostradas en el monográfico \"Perceptrons\" de Marvin Minsky y Seymour Papert en 1969). Por ejemplo, no son capaces de resolver algunos problemas triviales como el Exclusive OR (pero esto les pasa a todos los modelos lineales).\n",
    "\n",
    "Sin embargo, muchas de estas limitaciones se solventan haciendo stacking de multiples percetrones. La ANN resultante es llamada Multi-Layer Perceptron, y puede resolver problemas como el XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Perceptrón multicapa (MLP) y backpropagation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un MLP está compuesto de una capa input, una o mas capas TLUs (hidden layers) y una capa final de TLUs llamada output layer. Las primeras capas son llamadas lower layers, las últimas upper layers, cada capa excepto el output tiene un bias neuron y está totalmente conectada a la siguiente capa.\n",
    "\n",
    "Nota: La señal solo va en una dirección (de input a output), así que esta arquitectura es un feedforward neural network (FNN).\n",
    "\n",
    "Cuando una ANN (Artificial Neural Network) tiene un conjunto amplio de capas ocultas es llamada \"deep neural network\" (DNN). Estas son estudiadas por el campo de Deep Learning.\n",
    "\n",
    "El primer algoritmo que permitia entrenar MLPs fue el backpropagation. Es un Gradient Descent que usa un algoritmo eficiente para calcular los gradientes automáticamente: en dos pasos sobre la NN (uno forward y uno backward), el backpropagation calcula los gradientes del error de la red para cada peso de la red. En otras palabras, permite conocer qué parámetros de la red deben ser modificados para disminuir el error. Una vez sabe esto, hace un Gradient Descent típico y repite el algoritmo, hasta que converge en la solución.\n",
    "\n",
    "Nota: El cálculo automático de los gradientes se llama \"automatic differentiation\" o \"autodiff\". Existen varias técnicas de autodiff, cada una con pros y contras. La usada en backpropagation es la llamada \"reverse-mode autodiff\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar el backpropagation un poco más:\n",
    "\n",
    "    - Este algoritmo utiliza un mini-batch cada vez (por ejemplo con 32 instancias) y recorre el training set completo varias veces. Cada pasada es conocida como \"epoch\".\n",
    "    - Cada mini-batch se pasa a la input layer, que la deriva a la primera hidden layer. El algoritmo computa los outputs de cada neurona de la layer (para cada instancia del mini-batch) y se lo pasa a la siguiente, y así hasta llegar a la última capa, la output layer. Este es el \"forward pass\", que es similar a cómo se hacen las predicciones, salvo que se guarda cada resultado intermedio de la MLP.\n",
    "    - Después, el algoritmo mide el error de medida usando una loss function.\n",
    "    - En el siguiente paso, el algoritmo computa cuánto ha contribuido al error cada output connection. Esto se hace aplicando la regla de la cadena (que es rápido y preciso).\n",
    "    - Más tarde, el algoritmo computa cuánto de ese error viene de cada conexión en la capa anterior, usando la regla de la cadena otra vez. Este paso se repite recursivamente hasta que se ha calculado el error cometido por cada connection weight de la NN propagando el gradiente de error \"backward through the network\".\n",
    "    - Por último hace un Gradient Descent para modificar todos los pesos de las conexiones de la red usando el gradiente del error que acaba de calcular.\n",
    "    \n",
    "Resumiendo: para cada instancia el backpropagation hace una predicción, mide el error, hace un backward para medir la contribución de cada conexión y modifica los pesos de la red con un Gradient Descent.\n",
    "\n",
    "Existe un detalle importante, y es que es necesario inicializar todos los pesos de las conexiones de las capas ocultas de forma random. Si se inicializan todas las neuronas de la misma forma (por ejemplo, con cero) la red actuará como si solo existiese una capa al ser todas idénticas. Es necesario romper la simetría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que el algoritmo funcione correctamente se hizo un cambio en la arquitectura de las MLP: se reemplazó la función step por la logística (sigma(z)=1/1+exp(-z)). Esto se hizo porque la función step no permite hacer Gradient Descent, mientras que la función logística tiene derivada distinta de cero y definida para cada punto. \n",
    "\n",
    "Existen algunas funciones más que también se pueden usar como función de activación, por ejemplo:\n",
    "\n",
    "    - La función tangente hiperbolica tanh(z) = 2*sigma*(2*z)-1: Es S-Shaped, continua y diferenciable pero su rango de valores va de -1 a 1 (en lugar de 0 a 1 que toma la función logística). Esta función suele acelerar el proceso de convergencia al hacer que las capas se centren entorno a 0 al inicio del entrenamiento.\n",
    "    - La función Rectified Linear Unit (ReLU): ReLU(z) = max(0,z): Es continua pero no diferenciable en z=0 (la tangente cambia de forma abrupta ahí, haciendo que el Gradient Descent pueda rebotar), y su derivada es 0 para z<0. En la práctica funciona bastante bien, y tiene la ventaja de que se computa mucho más rápido. Además. el hecho de no tener un valor máximo a veces evita problemas de Gradient Descent, pero esto ya lo veremos en otros notebooks.\n",
    "    \n",
    "El motivo inicial por el que necesitamos una función de activación es que las capas actuan de forma lineal. Si no se pone algún tipo de relación no lineal entre capas, da igual cuantas capas se pongan que será como poner una sola. Vamos a ver gráficamente como son las funciones de arriba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MLPs de Regresión y Clasificación</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las MLP se pueden utilizar para multitud de tareas, tanto de regresión como de clasificación. Vamos a ver cómo se utilizan para cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MLPs de Regressión</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagina que quieres predecir un valor (por ejemplo un precio de un activo), en este caso solo necesitas una neurona output con el valor predicho. Para regresión multivariante (predecir varios valores) necesitarás más neuronas en la capa output, una por cada valor que se quiera predecir.\n",
    "\n",
    "En general, cuando se hace un MLP para regresión no se quiere usar función de activación para las neuronas output, para que puedan tomar cualquier valor. Solo en los casos en los que el output tenga alguna restricción (p.e. deba ser positivo), se tomará alguna de estas funciones (en el ejemplo, la ReLU o la softplus).\n",
    "\n",
    "La función de pérdida que se suele usar es la mean squared error. En casos especiales se usan otras, como cuando tienes muchos atípicos y usas la función de mean absolute error, o la Huber loss, que es una combinación de las dos. Si el output puede tomar un rango determinado de valores se usan la Logistic o la Tanh.\n",
    "\n",
    "Resumiendo los distintos hiperparámetros que tendria la MLP:\n",
    "    - Neuronas input: una por cada input feature.\n",
    "    - Capas ocultas: depende del problema (suele ser 1-5).\n",
    "    - Neuronas por cada capa oculta: depende del problema (suele ser 10-100).\n",
    "    - Neuronas output: 1 por cada dimensión predicha.\n",
    "    - Función de activación de las capas ocultas: ReLU (o SELU).\n",
    "    - Función de activación de la capa output: ninguna o ReLU/SoftPLUS o Logistic/Tanh.\n",
    "    - Loss function: MSE o MAE/Huber."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MLPs de Clasificación</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los MLP también se pueden usar para clasificación. Para problemas binarios, sólo necesitas una neurona output usando una función de activación Logistic (así dará algo cercano a 0 o 1). La probabilidad de que pertenezca a la clase es de la probabilidad obtenida (o de 1-prob para la otra clase).\n",
    "\n",
    "Con MLP también se pueden estudiar sistemas binarios multilabel. Por ejemplo filtros de spam y si a la vez es urgente o no. En este caso se necesitan dos neuronas output funcionando como la del ejemplo anterior. En general necesitas una neurona para cada clase a predecir. \n",
    "\n",
    "En casos en los que las clases tengan mas de una posibilidad (por ejemplo números del 0 al 9), llamado multiclass classification, se necesita una neurona output para cada posibilidad, y utilizar la función softmax para determinar, ya que son clases exclusivas.\n",
    "\n",
    "En cuanto a loss function se suele usar la cross-entropy.\n",
    "\n",
    "Resumiendo los hiperparámetros:\n",
    "    - Input y capas ocultas: como en regresión.\n",
    "    - Número de neuronas output: 1 en binario, 1 por etiqueta en mutilabel, 1 por clase en multiclass.\n",
    "    - Función de activación output: Logistic en binario y multilabel, Softmax en multiclass.\n",
    "    - Loss function: Cross-Entropy.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementación de MLP con Keras. Construcción de distintos modelos. Guardado. Uso de callbacks y visualización</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras es una API de Deep Learning a alto nivel que permite construir, entrenar, evaluar y ejecutar todo tipo de redes neuronales. La documentación está en https://keras.io/ y su implementación en https://github.com/keras-team/keras . Es muy popular porque es fácil de usar, y para la computación se sirve de TensorFlow, Microsift Cognitive Toolkit (CNTK) o Theano. \n",
    "\n",
    "Desde 2016 existen otras implementaciones de Keras. Ahora se puede ejecutar desde Apache MXNet, Apple's Core ML, Javascript o Typescript (para correrlo en un entorno web) o PlaidML (para ejecutarlo en cualquier GPU, no solo Nvidia). Además TensorFlow ahora trae su Keras propio, llamado tf.keras que sólo soporta TensorFlow como Backend pero tiene features útiles. Por esto vamos a usar esta implementación en este y en los siguientes Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Construcción de un clasificador de imagen</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir el primer modelo con Keras, usando el dataset fashion MNIST, que tiene imágenes de ropa en lugar de números pero conserva el mismo formato que el MNIST (70000 imágenes en blanco y negro de 28x28 y en 10 clases).\n",
    "\n",
    "Este dataset viene cargado en keras por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamaño del dataset (60000, 28, 28)\n",
      "tipo del dataset: uint8\n"
     ]
    }
   ],
   "source": [
    "print('tamaño del dataset',X_train_full.shape)\n",
    "print('tipo del dataset:',X_train_full.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de clase:  Coat\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear un training y un test:\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Ponemos las clases:\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# Ejemplo:\n",
    "print('Ejemplo de clase: ',class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir nuestra primera NN! Esta vez va a tener, por ejemplo, dos hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0306 18:08:07.230985 140224505124672 deprecation.py:506] From /home/kiko/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model layers:  [<tensorflow.python.keras.layers.core.Flatten object at 0x7f87f9f6ca50>, <tensorflow.python.keras.layers.core.Dense object at 0x7f87f9f28fd0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f87f9ee1850>, <tensorflow.python.keras.layers.core.Dense object at 0x7f87f9ee16d0>]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "El tamaño de weights: (784, 300)\n",
      "Ejemplo de weights: [[ 1.51395276e-02 -1.82861090e-03  6.46853000e-02 ...  4.11476493e-02\n",
      "   4.48341668e-03  6.43212348e-02]\n",
      " [ 7.06813633e-02 -7.17071518e-02  2.27016509e-02 ... -2.19323486e-03\n",
      "  -6.53055459e-02  5.23079932e-02]\n",
      " [ 8.80487263e-04 -4.33592796e-02  2.48449892e-02 ... -1.81251951e-02\n",
      "  -4.38203737e-02  5.82965463e-03]\n",
      " ...\n",
      " [ 4.40037698e-02 -4.57630977e-02  2.55986676e-02 ...  6.15458190e-02\n",
      "   6.99604601e-02 -4.44529243e-02]\n",
      " [-2.62218677e-02 -3.79119292e-02 -4.80706654e-02 ... -4.56680208e-02\n",
      "   5.61409146e-02  6.07226491e-02]\n",
      " [ 6.80729002e-03  7.21444786e-02  1.13184527e-02 ...  1.09905675e-02\n",
      "  -1.83288790e-02 -6.35534525e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Esta línea crea un modelo Sequential, que es el marco más simple de NN apiladas\n",
    "model = keras.models.Sequential()\n",
    "# Esta línea ñade al modelo la primera capa. Computa X, hace X.reshape(-1,1). No tiene parámetros porque solo\n",
    "# hace preprocessing y sirve como capa input al proceso.\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28])) \n",
    "# Ahora se añaden las 2 capas intermedias con función ReLU:\n",
    "model.add(keras.layers.Dense(300,activation='relu'))\n",
    "model.add(keras.layers.Dense(100,activation='relu'))\n",
    "# Como capa final se añade una capa densa con 10 neuronas (una para cada clase) y función softmax:\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "# En realidad todo esto se suele escribir de una forma más correcta, en lugar de añadir capas una a una se hace\n",
    "# todo de una vez. Primero limpiamos para que no sobreescriba el modelo:\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Y creamos:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Las model layer son:\n",
    "\n",
    "print('model layers: ',model.layers)\n",
    "\n",
    "# Y el summary del modelo es:\n",
    "print(model.summary())\n",
    "\n",
    "# Ademas se pueden conseguir los weights y los biases de cada capa:\n",
    "\n",
    "print('')\n",
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()\n",
    "print('El tamaño de weights:', weights.shape)\n",
    "print('Ejemplo de weights:', weights[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ve, las Dense layer están ya inicializadas por defecto de forma random, para romper la simetría y que sirvan a la hora de entrenar. Si se quiere inicializar de forma distinta, se puede usar el kernel_initialiter o bias_initialiter en los hiperparámetros.\n",
    "\n",
    "Si no se inicializa la capa input con input_shape, keras espera hasta que entrenes para definir ese tamaño.\n",
    "\n",
    "\n",
    "Los ejemplos de código documentados en keras.io también funcionan con tf.keras. Para ello basta con cambiar los imports, por ejemplo de keras:\n",
    "\n",
    "    from keras.layers import Dense\n",
    "    output_layer = Dense(10)\n",
    "    \n",
    "De tensorflow:\n",
    "    \n",
    "    from tensorflow.keras.layers import Dense\n",
    "    output_layer = Dense(10)\n",
    "    \n",
    "    from tensorflow import keras\n",
    "    output_layer = keras.layers.Dense(10)\n",
    "    \n",
    "Los dos ejemplos de tensorflow son válidos.\n",
    "\n",
    "\n",
    "\n",
    "Arriba hemos visto también el summary de los modelos. Esa función muestra los distintos nombres de capas y sus parámetros entrenables y no entrenables. En nuestro ejemplo todos los parámetros son entrenables, y las capas densas tienen muchísimos parámetros. Esto hace que el modelo sea muy flexible entrenando los datos, lo que puede dar lugar a overfitting, sobre todo si no tienes demasiados datos.\n",
    "\n",
    "Existen otros atributos como .layers, .get_weights() o .shape que te muestran las capas o los pesos y su tamaño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se crea un modelo, este se compila usando la función compile(). Esto permite definir la loss function y el optimizer a usar. También es posible especificar una lista de métricas extra a computar durante el entrenamiento y la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0306 18:08:08.288309 140224505124672 deprecation.py:506] From /home/kiko/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:127: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Se compila el modelo:\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Otra forma equivalente es:\n",
    "\n",
    "#model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "#              optimizer=keras.optimizers.SGD(),\n",
    "#              metrics=[keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al compilar se ha utilizado \"sparse_categorical_crossentropy\" porque se tienen etiquetas unas pocas etiquetas y son exclusivas (solo se puede tener un target index, de 0 a 9 en este caso). Si se necesitase saber la probabilidad por clase se podria usar \"categorical_crossentropy\". Si fuese clasificación binaria en lugar de multietiqueta, se usaria la función sigmoid en ligar de la softmax y habría que poner \"binary_crossentropy\".\n",
    "\n",
    "Después, se ha utilizado el optimizer \"sgd\" para entrenar el modelo usando un Stochastic Gradient Descent. Con esto keras usará el algoritmo que hemos explicado mas arriba (reverse-mode autodiff + Gradient Descent). En otros notebooks veremos otros optimizers.\n",
    "\n",
    "Por último, como es un classifier, la métrica a usar es \"accuracy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha compilado el modelo está listo para ser entrenado. Para ello sólo hace falta utilizar el método .fit(). Se pasa el input, el target y el número de epochs a entrenar (por defecto es 1, que no parece buena idea). De forma opcional se puede pasar un validation set, y keras computará la pérdida y la métrica extra al final de cada epoch. Esto permite controlar el overfitting, que sabremos que está ocurriendo si el performance en el training set es mucho mejor que en el validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 3s 53us/sample - loss: 1.5064 - acc: 0.5600 - val_loss: 1.0095 - val_acc: 0.6850\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.8802 - acc: 0.7099 - val_loss: 0.7741 - val_acc: 0.7506\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.7353 - acc: 0.7581 - val_loss: 0.6813 - val_acc: 0.7848\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.6636 - acc: 0.7833 - val_loss: 0.6248 - val_acc: 0.8012\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.6160 - acc: 0.7990 - val_loss: 0.5861 - val_acc: 0.8158\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.5820 - acc: 0.8101 - val_loss: 0.5580 - val_acc: 0.8222\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5564 - acc: 0.8171 - val_loss: 0.5358 - val_acc: 0.8312\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5364 - acc: 0.8221 - val_loss: 0.5178 - val_acc: 0.8344\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.5201 - acc: 0.8268 - val_loss: 0.5033 - val_acc: 0.8388\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.5068 - acc: 0.8302 - val_loss: 0.4946 - val_acc: 0.8410\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4954 - acc: 0.8327 - val_loss: 0.4808 - val_acc: 0.8464\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.4857 - acc: 0.8358 - val_loss: 0.4757 - val_acc: 0.8448\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 3s 50us/sample - loss: 0.4770 - acc: 0.8374 - val_loss: 0.4697 - val_acc: 0.8492\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.4698 - acc: 0.8403 - val_loss: 0.4596 - val_acc: 0.8524\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.4629 - acc: 0.8415 - val_loss: 0.4544 - val_acc: 0.8532\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4567 - acc: 0.8435 - val_loss: 0.4483 - val_acc: 0.8530\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.4510 - acc: 0.8452 - val_loss: 0.4446 - val_acc: 0.8520\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4462 - acc: 0.8465 - val_loss: 0.4406 - val_acc: 0.8562\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4413 - acc: 0.8479 - val_loss: 0.4350 - val_acc: 0.8578\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4367 - acc: 0.8499 - val_loss: 0.4336 - val_acc: 0.8586\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4328 - acc: 0.8515 - val_loss: 0.4267 - val_acc: 0.8618\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4287 - acc: 0.8528 - val_loss: 0.4255 - val_acc: 0.8604\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4250 - acc: 0.8535 - val_loss: 0.4214 - val_acc: 0.8580\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4216 - acc: 0.8533 - val_loss: 0.4216 - val_acc: 0.8584\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 3s 48us/sample - loss: 0.4181 - acc: 0.8556 - val_loss: 0.4156 - val_acc: 0.8606\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 3s 47us/sample - loss: 0.4152 - acc: 0.8555 - val_loss: 0.4182 - val_acc: 0.8606\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.4121 - acc: 0.8573 - val_loss: 0.4129 - val_acc: 0.8626\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 3s 54us/sample - loss: 0.4087 - acc: 0.8588 - val_loss: 0.4096 - val_acc: 0.8616\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 3s 49us/sample - loss: 0.4062 - acc: 0.8597 - val_loss: 0.4068 - val_acc: 0.8662\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.4036 - acc: 0.8605 - val_loss: 0.4047 - val_acc: 0.8672\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos puesto un validation set, se pueden comparar los resultados en el training y en el validation. Al final de cada epoch keras muestra el número de instances procesadas y las métricas en training y val. En el ejemplo hemos tenido una accuracy de 86,72% en el validation, nada mal. Otra forma, sin pasar validation_data, se puede usar validation_split con el ratio del set a usar como validation (p.e. 0.1).\n",
    "\n",
    "Si el training data fuese muy skewed habria clases sobrerrepresentadas sobre otras. Se podría usar el argumento class_weight cuando se haga el .fit(), para dar mayor peso a las clases infrarrepresentadas y menor peso a las sobrerrepresentadas. Estos pesos se tienen en cuenta a la hora de computar la pérdida. Si existen ciertas instances mas importantes (por ejemplo, etiquetadas por expertos), se puede usar el argumento sample_weight para darle mas pesos. Se puede dar como argumento los sample weights metiendolos como un tercer item en el validation_data tuple.\n",
    "\n",
    "El método .fit() devuelve un histórico con los parámetros de entrenamiento, la lista de las epochs y un diccionario con las métricas. Vamos a ver un plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros:  {'batch_size': 32, 'epochs': 30, 'steps': None, 'samples': 55000, 'verbose': 0, 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc']}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parámetros:\n",
    "print('Parámetros: ',history.params)\n",
    "\n",
    "# Plots:\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve en la gráfica que las curvas de train y val van bastante juntas. Eso es señal de que no hay overfitting. El hecho de que haya mejor performance sobre el val es pura casualidad. Lo normal es que, si se alarga el entrenamiento, el training set tenga una mejor performance.\n",
    "\n",
    "Se ve que las métricas seguian mejorando. Esto nos indica que sería una buena idea seguir entrenándolo. Es posible simplemente llamando al .fit() otra vez, porque keras lo retoma donde lo dejó.\n",
    "\n",
    "Si no estas contento con el performance debes volver atras y cambiar los hiperparámetros (p.e. el número de capas, de neuronas por capa, la función de activación, el número de epochs o el batch size). Esto lo veremos más adelante.\n",
    "\n",
    "Una vez estes contento con el accuracy de tu validation, lo suyo es evaluarlo en el test antes de ponerlo en producción. Para ello se usa el método evaluate() (que además soporta argumentos como batch_size o sample_weight).\n",
    "\n",
    "\n",
    "Nota: NUNCA se usa el test set como referencia para los hiperparámetros!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 29us/sample - loss: 76.9105 - acc: 0.8106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[76.91052300434113, 0.8106]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya se puede usar el modelo para hacer predicciones! Para ello basta con usar el método .predict() sobre nuevas instances: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso nos ha dado probabilidades del 100%, pero podrían haber salido dispersas entre varias clases. Si lo que se quiere es obtener una única prediccion, la de la clase, se debe usar el método .predict_classes():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las predicciones son: [9 2 1]\n",
      "Los valores reales eran: [9 2 1]\n",
      "Y las clases son: ['Ankle boot' 'Pullover' 'Trouser']\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "print('Las predicciones son:',y_pred)\n",
    "print('Los valores reales eran:',y_test[:3])\n",
    "print('Y las clases son:', np.array(class_names)[y_pred])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Construcción de un MLP de regresión con la Sequential API</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estudiar el problema de regresión vamos a usar el dataset de California housing pricing. Para esto vamos a usar la version de SKlearn fetch_california_housing(), que solo tiene las features numéricas y no tiene missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el dataset\n",
    "housing = fetch_california_housing()\n",
    "# Separamos en train, test y valid:\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "# Escalamos los datos con el Scaler:\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un modelo de regresión con MLP es similar al de clasificación hecho. La diferencia principal es que la última capa está formada por una única neurona porque solo queremos predecir un valor, y no tiene ninguna función de activación y la loss function es el mean squared error. Como el dataset es un poco malo, tiene mucho ruido, vamos a usar solo una hidden layer para evitar overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 2.7227 - val_loss: 1.2091\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.9287 - val_loss: 0.8145\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.7439 - val_loss: 0.7193\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.6839 - val_loss: 0.6838\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.6470 - val_loss: 0.6433\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.6197 - val_loss: 0.6198\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5949 - val_loss: 0.5946\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5736 - val_loss: 0.5741\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5554 - val_loss: 0.5641\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5382 - val_loss: 0.5466\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5234 - val_loss: 0.5322\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.5102 - val_loss: 0.5176\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.5001 - val_loss: 0.5096\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4892 - val_loss: 0.5007\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4811 - val_loss: 0.4930\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4728 - val_loss: 0.4846\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.4661 - val_loss: 0.4775\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.4599 - val_loss: 0.4729\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4537 - val_loss: 0.4704\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4481 - val_loss: 0.4627\n",
      "5160/5160 [==============================] - 0s 20us/sample - loss: 0.4905\n"
     ]
    }
   ],
   "source": [
    "# Establecemos semilla:\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Creamos la estructura de la NN\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation=\"relu\",input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compilamos el modelo:\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "\n",
    "# Creamos el histórico, predecimos y vemos como funciona en unos cuantos registros:\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lo pintamos a ver que tal:\n",
    "\n",
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modelos complejos usando la Functional API: Wide & Deep Neural Network</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de NN no secuencial es la Wide & Deep NN. Esta se divide en dos partes, una deep NN clásica y una capa que conecta directamente los inputs con la capa output. Con esto se consigue que la NN aprenda patrones complejos (a partir de la parte deep NN) y patrones simples (a partir de la conexion directa).\n",
    "\n",
    "Vamos a construir esto para el dataset de California:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 30)           270         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           930         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Establecemos semilla:\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Hacemos el Wide & Deep:\n",
    "\n",
    "# Primero creamos el input object. Hacemos esto asi porque luego tendremos varios inputs:\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "\n",
    "# Creamos dos capas hidden, dense y de 30 neuronas y con ReLu. Además las conectamos con la capa anterior. Esto\n",
    "# es la Functional API, se pasa el input como si fuese una función.\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "# Creamos un objeto concatenate que concatene el input y el output de la segunda hidden layer.\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "\n",
    "# Creamos la capa output, dense y con una única neurona y sin función de activacion.\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "# Por último, se crea el modelo Keras con el input y el output\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "# Veamos el summary:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 1.9005 - val_loss: 0.7827\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.6932 - val_loss: 0.7261\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.6331 - val_loss: 0.6446\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.6036 - val_loss: 0.6349\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5788 - val_loss: 0.5889\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5605 - val_loss: 0.6011\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5432 - val_loss: 0.5687\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5296 - val_loss: 0.5528\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.5191 - val_loss: 0.5798\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5090 - val_loss: 0.5476\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.5016 - val_loss: 0.5466\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4933 - val_loss: 0.5110\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4884 - val_loss: 0.5259\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4805 - val_loss: 0.5001\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4745 - val_loss: 0.5191\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4684 - val_loss: 0.4908\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4639 - val_loss: 0.5034\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4598 - val_loss: 0.4911\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.4548 - val_loss: 0.5081\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.4499 - val_loss: 0.4809\n",
      "5160/5160 [==============================] - 0s 24us/sample - loss: 0.4987\n"
     ]
    }
   ],
   "source": [
    "# Hagamos una predicción con el modelo. Para ello compile, fit, evaluate y predict:\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede suceder que se quiera mandar, a la vez, un subset por el deep NN y otro por el path directo. Para ello se crean múltiples inputs. Si, por ejemplo, se quieren mandar un subset de variables por cada path (en el ejemplo son variables 0-4 por el deep, variables 2-7 por el wide) sería así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 2.3558 - val_loss: 1.1897\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.8680 - val_loss: 0.8099\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.7173 - val_loss: 0.7172\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.6631 - val_loss: 0.6746\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.6276 - val_loss: 0.6394\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.5999 - val_loss: 0.6165\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.5773 - val_loss: 0.5969\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5586 - val_loss: 0.5804\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.5438 - val_loss: 0.5696\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.5307 - val_loss: 0.5593\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.5200 - val_loss: 0.5481\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.5098 - val_loss: 0.5357\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.5014 - val_loss: 0.5295\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4929 - val_loss: 0.5198\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4860 - val_loss: 0.5155\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4791 - val_loss: 0.5069\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4727 - val_loss: 0.5027\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4675 - val_loss: 0.4973\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4614 - val_loss: 0.4959\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4557 - val_loss: 0.4878\n",
      "5160/5160 [==============================] - 0s 26us/sample - loss: 0.5063\n"
     ]
    }
   ],
   "source": [
    "# Establecemos semilla:\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Con variables distintas por cada path:\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "# Predecimos:\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "\n",
    "# Al hacer el fit, evaluate o predict, hay que pasar el par de matrices distintas:\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También puede ocurrir que se necesiten múltiples outputs, por ejemplo:\n",
    "\n",
    "    - Si la tarea lo exige: por ejemplo si se quiere encontrar y localizar un objeto en una imagen, es a la vez una tarea de clasificación del objeto y de regresión (encontrar las coordenadas del centro del objeto.\n",
    "    - Si se realizan distintas tareas independientes sobre los mismos datos. Se puede entrenar una NN para cada tarea, pero suele dar mejores resultados hacer todas las tareas sobre una misma NN con un output por tarea, porque la NN puede aprender patrones comunes entre distintas tareas.\n",
    "    - Como técnica de regularización: por ejemplo, se pueden añadir outputs auxiliares para asegurar que las capas bajas de la NN aprenden patrones sin ceder todo el peso a las capas superiores. Con este tipo de técnicas se consigue que el modelo generalice mejor y se reduce el overfitting.\n",
    "    \n",
    "Añadir outputs extra es tan sencillo como conectarlos en las capas que se desee, y añadirlos a la lista de outputs. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 2.7219 - main_output_loss: 2.4435 - aux_output_loss: 5.2272 - val_loss: 1.3539 - val_main_output_loss: 1.0704 - val_aux_output_loss: 3.9055\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 1.0331 - main_output_loss: 0.8314 - aux_output_loss: 2.8485 - val_loss: 0.9393 - val_main_output_loss: 0.7383 - val_aux_output_loss: 2.7479\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.7978 - main_output_loss: 0.6611 - aux_output_loss: 2.0274 - val_loss: 0.8126 - val_main_output_loss: 0.6450 - val_aux_output_loss: 2.3214\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.7145 - main_output_loss: 0.6038 - aux_output_loss: 1.7108 - val_loss: 0.7504 - val_main_output_loss: 0.6086 - val_aux_output_loss: 2.0264\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.6682 - main_output_loss: 0.5694 - aux_output_loss: 1.5576 - val_loss: 0.7023 - val_main_output_loss: 0.5759 - val_aux_output_loss: 1.8404\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.6402 - main_output_loss: 0.5487 - aux_output_loss: 1.4637 - val_loss: 0.6706 - val_main_output_loss: 0.5624 - val_aux_output_loss: 1.6447\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.6160 - main_output_loss: 0.5292 - aux_output_loss: 1.3969 - val_loss: 0.6394 - val_main_output_loss: 0.5426 - val_aux_output_loss: 1.5106\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5971 - main_output_loss: 0.5141 - aux_output_loss: 1.3447 - val_loss: 0.6165 - val_main_output_loss: 0.5278 - val_aux_output_loss: 1.4145\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5826 - main_output_loss: 0.5024 - aux_output_loss: 1.3043 - val_loss: 0.6138 - val_main_output_loss: 0.5326 - val_aux_output_loss: 1.3441\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5693 - main_output_loss: 0.4915 - aux_output_loss: 1.2698 - val_loss: 0.5983 - val_main_output_loss: 0.5209 - val_aux_output_loss: 1.2941\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5592 - main_output_loss: 0.4833 - aux_output_loss: 1.2419 - val_loss: 0.5839 - val_main_output_loss: 0.5093 - val_aux_output_loss: 1.2561\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5488 - main_output_loss: 0.4745 - aux_output_loss: 1.2169 - val_loss: 0.5634 - val_main_output_loss: 0.4897 - val_aux_output_loss: 1.2266\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5420 - main_output_loss: 0.4695 - aux_output_loss: 1.1942 - val_loss: 0.5650 - val_main_output_loss: 0.4940 - val_aux_output_loss: 1.2039\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5339 - main_output_loss: 0.4628 - aux_output_loss: 1.1733 - val_loss: 0.5509 - val_main_output_loss: 0.4803 - val_aux_output_loss: 1.1860\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5279 - main_output_loss: 0.4582 - aux_output_loss: 1.1550 - val_loss: 0.5545 - val_main_output_loss: 0.4863 - val_aux_output_loss: 1.1674\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5216 - main_output_loss: 0.4534 - aux_output_loss: 1.1356 - val_loss: 0.5408 - val_main_output_loss: 0.4728 - val_aux_output_loss: 1.1525\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.5171 - main_output_loss: 0.4502 - aux_output_loss: 1.1185 - val_loss: 0.5412 - val_main_output_loss: 0.4750 - val_aux_output_loss: 1.1371\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5119 - main_output_loss: 0.4464 - aux_output_loss: 1.1019 - val_loss: 0.5400 - val_main_output_loss: 0.4752 - val_aux_output_loss: 1.1232\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5071 - main_output_loss: 0.4428 - aux_output_loss: 1.0861 - val_loss: 0.5431 - val_main_output_loss: 0.4802 - val_aux_output_loss: 1.1088\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5030 - main_output_loss: 0.4400 - aux_output_loss: 1.0702 - val_loss: 0.5289 - val_main_output_loss: 0.4659 - val_aux_output_loss: 1.0967\n",
      "5160/5160 [==============================] - 0s 33us/sample - loss: 0.5488 - main_output_loss: 0.4840 - aux_output_loss: 1.1323\n"
     ]
    }
   ],
   "source": [
    "# Establecemos semilla:\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Modelo con regularización que añade outputs en la capa hidden2:\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])\n",
    "\n",
    "# Compilamos y entrenamos/evaluamos. Para compilar se debe añadir una loss function ara cada output:\n",
    "\n",
    "# Además, se puede dar distinto peso a cada métrica. Por defecto Keras hace 50/50.\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "# Se deben pasar labels para cada output, así que en este caso hay que poner 2 veces el y_train:\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "# La pérdida total es una conjunción de ambas, y Keras nos devuelve ambas y la total:\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "# Con las predicciones tambien nos devuelve ambas:\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha visto, entrenar cualquier tipo de arquitectura de NN es tan sencillo como añadir las capas correspondientes y conectarlas en el lugar que corresponda!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modelos dinámicos usando la Subclassing API</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto la Sequential API como la Functional API son declarativas, por ello se empieza definiendo las capas que vas a usar y cómo están conectadas, y después ya puedes entrenar el modelo. Esto tiene ventajas como que los modelos pueden ser guardados, clonados, compartidos, analizados, se pueden inferir capas y check types permitiendo encontrar los errores rápido. Además es muy fácil de debuggear porque el modelo es un grafo estático de capas. Pero su principal contra es eso, que es estático. Algunos modelos necesitan recursividad (como bucles), capas condicionales, tamaños que cambian y otros comportamientos dinámicos. Para eso, o porque prefieres más libertad a la hora de programar, se utiliza la Subclassing API.\n",
    "\n",
    "Es tan simple como hacer una subclase de la clase Model, crear las capas que se necesitan en el constructor y usarlas para realizar las computaciones que se quieran con el método call(). Por ejemplo, para el WideAndDeepModel de antes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs) # Esto hereda los argumentos del __init__ de Model, como por ej name\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es muy parecido a la Functional API, pero no necesitamos crear los inputs porque solo estamos definiendo la clase. Se meterán los inputs cuando se llame al método call(), y podemos separar la creacion de las layers en el constructor de su uso en el método call(). Esto permite hacer lo que quieras en el método call(), ya sea bucles, ifs, operaciones o lo que quieras.\n",
    "\n",
    "La contraparte a esto es que tu arquitectura del modelo estará oculta bajo el método call(), por lo que Keras no puede inspeccionarla, guardarla, clonarla o sacar información de las layers cuando hagas el summary(). De hecho, summary() solo muestra una lista de capas, sin información de las mismas o de las conexiones entre ellas (esto es derivado de que, al no definir el input al crearlas, keras no es capaz de recorrer la estructura). Por eso, si no se necesita una flexibilidad extra, es mejor usar las API Sequential o Functional.\n",
    "\n",
    "Nota: Es posible usar los modelos de Keras como si fuesen una única capa, por lo que se pueden componer entre si para crear arquitecturas complejas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Guardado y restaurado de un modelo Keras</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar un modelo Keras es tan simple como usar la función .save() La única condición es que deben ser modelos de API Functional o Sequential, nunca Subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 1.9937 - main_output_loss: 1.7825 - aux_output_loss: 3.8944 - val_loss: 1.4750 - val_main_output_loss: 1.2898 - val_aux_output_loss: 3.1416\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 1.0243 - main_output_loss: 0.8536 - aux_output_loss: 2.5601 - val_loss: 1.0984 - val_main_output_loss: 0.9688 - val_aux_output_loss: 2.2643\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.8688 - main_output_loss: 0.7472 - aux_output_loss: 1.9631 - val_loss: 0.9229 - val_main_output_loss: 0.8223 - val_aux_output_loss: 1.8279\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.7901 - main_output_loss: 0.6913 - aux_output_loss: 1.6791 - val_loss: 0.8253 - val_main_output_loss: 0.7365 - val_aux_output_loss: 1.6239\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.7390 - main_output_loss: 0.6496 - aux_output_loss: 1.5432 - val_loss: 0.7597 - val_main_output_loss: 0.6752 - val_aux_output_loss: 1.5206\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.7008 - main_output_loss: 0.6153 - aux_output_loss: 1.4697 - val_loss: 0.7170 - val_main_output_loss: 0.6346 - val_aux_output_loss: 1.4587\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.6693 - main_output_loss: 0.5858 - aux_output_loss: 1.4212 - val_loss: 0.6835 - val_main_output_loss: 0.6020 - val_aux_output_loss: 1.4165\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.6421 - main_output_loss: 0.5598 - aux_output_loss: 1.3834 - val_loss: 0.6563 - val_main_output_loss: 0.5755 - val_aux_output_loss: 1.3837\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.6187 - main_output_loss: 0.5371 - aux_output_loss: 1.3525 - val_loss: 0.6349 - val_main_output_loss: 0.5549 - val_aux_output_loss: 1.3542\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5979 - main_output_loss: 0.5174 - aux_output_loss: 1.3228 - val_loss: 0.6157 - val_main_output_loss: 0.5367 - val_aux_output_loss: 1.3269\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5799 - main_output_loss: 0.5003 - aux_output_loss: 1.2962 - val_loss: 0.5992 - val_main_output_loss: 0.5211 - val_aux_output_loss: 1.3019\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5643 - main_output_loss: 0.4858 - aux_output_loss: 1.2704 - val_loss: 0.5845 - val_main_output_loss: 0.5075 - val_aux_output_loss: 1.2780\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5510 - main_output_loss: 0.4738 - aux_output_loss: 1.2459 - val_loss: 0.5733 - val_main_output_loss: 0.4975 - val_aux_output_loss: 1.2557\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5392 - main_output_loss: 0.4633 - aux_output_loss: 1.2220 - val_loss: 0.5630 - val_main_output_loss: 0.4884 - val_aux_output_loss: 1.2343\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5298 - main_output_loss: 0.4552 - aux_output_loss: 1.2013 - val_loss: 0.5553 - val_main_output_loss: 0.4823 - val_aux_output_loss: 1.2124\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5214 - main_output_loss: 0.4482 - aux_output_loss: 1.1796 - val_loss: 0.5482 - val_main_output_loss: 0.4767 - val_aux_output_loss: 1.1924\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5143 - main_output_loss: 0.4426 - aux_output_loss: 1.1591 - val_loss: 0.5433 - val_main_output_loss: 0.4734 - val_aux_output_loss: 1.1729\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5082 - main_output_loss: 0.4380 - aux_output_loss: 1.1402 - val_loss: 0.5390 - val_main_output_loss: 0.4707 - val_aux_output_loss: 1.1542\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5025 - main_output_loss: 0.4337 - aux_output_loss: 1.1213 - val_loss: 0.5363 - val_main_output_loss: 0.4697 - val_aux_output_loss: 1.1365\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4970 - main_output_loss: 0.4298 - aux_output_loss: 1.1025 - val_loss: 0.5321 - val_main_output_loss: 0.4670 - val_aux_output_loss: 1.1176\n",
      "5160/5160 [==============================] - 0s 30us/sample - loss: 0.5444 - main_output_loss: 0.4752 - aux_output_loss: 1.1675\n"
     ]
    }
   ],
   "source": [
    "# El ejemplo de antes:\n",
    "\n",
    "# Establecemos semilla:\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Modelo con regularización que añade outputs en la capa hidden2:\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])\n",
    "\n",
    "# Compilamos y entrenamos/evaluamos. Para compilar se debe añadir una loss function ara cada output:\n",
    "\n",
    "# Además, se puede dar distinto peso a cada métrica. Por defecto Keras hace 50/50.\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "# Se deben pasar labels para cada output, así que en este caso hay que poner 2 veces el y_train:\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "# La pérdida total es una conjunción de ambas, y Keras nos devuelve ambas y la total:\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "# Con las predicciones tambien nos devuelve ambas:\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "\n",
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras guardará la arquitectura del modelo (incluyendo los hiperparámetros de cada capa) y los valores de todo el modelo para cada capa (los pesos de conexiones y los biases) usando el formato HDF5. Además guarda el optimizer y sus hiperarámetros (y su estado).\n",
    "\n",
    "Lo típico es tener un script que crea el modelo y lo guarda, y uno o mas scripts (o web services) que lo cargan y lo usan para predecir. Cargar modelos es igual de simple que guardar, basta con usar la función load_model():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0306 18:10:34.250461 140224505124672 deprecation.py:506] From /home/kiko/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model= keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no se pueda usar para la Subclassing API, se pueden usar las funciones save_weights() y load_weights() para al menos guardar y restaurar los parámetros del modelo (pero todo lo demas lo tienes que guardar y restaurar por tu cuenta).\n",
    "\n",
    "En el caso (bastante común) de que el entrenamiento dure horas, no solo es necesario guardar el modelo al final del entrenamiento, si no que es conveniente guardar checkpoints a distintos intervalos del entrenamiento. Para decirle al método fit() que guarde checkpoints se usan los callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Uso de Callbacks</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método fit() permite un argumento callbacks, que permite poner una lista de objetos que Keras hará durante el entrenamiento (al principio y al final del entrenamiento, al principio y al final de cada epoch o incluso despues de procesar cada batch). Por ejemplo, el callback ModelCheckpoing guarda checkpoints de tu modelo a intervalos regulares durante el entrenamiento (por defecto al principio y al final de cada epoch).\n",
    "\n",
    "Además permite funcionalidades como el argumento save_best_only=True, que guardará el modelo solo cuando el desempeño en el validation set es mejor que el ya guardado. Esto es una forma muy simple de immplementar early stopping y evitar el overfitting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 1.5410 - val_loss: 0.7476\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.6487 - val_loss: 0.6214\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.5806 - val_loss: 0.5781\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.5412 - val_loss: 0.5516\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5129 - val_loss: 0.5236\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4908 - val_loss: 0.5131\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4737 - val_loss: 0.4945\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4595 - val_loss: 0.4818\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4480 - val_loss: 0.4759\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.4379 - val_loss: 0.4674\n",
      "5160/5160 [==============================] - 0s 30us/sample - loss: 0.4882\n"
     ]
    }
   ],
   "source": [
    "# Limpiamos de modelos y establecemos semilla\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Creamos un modelo genérico para enseñar el callback:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Ejemplo de Checkpoint cada epoch y que guarde el mejor modelo:\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "\n",
    "# Rollback al mejor modelo\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de hacer el early stopping es con el callback EarlyStopping. Esto detiene el entrenamiento cuando ya no se mejore en el validation set por un número de epochs definido en el argumento patience, y opcionalmente hace rollback al mejor modelo. Se pueden combinar ambos rollback para hacer checkpoints (por si tu ordenador crashea) e interrumpir el entrenamiento cuando ya no haya mejoras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4292 - val_loss: 0.4592\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4217 - val_loss: 0.4479\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4150 - val_loss: 0.4445\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.4091 - val_loss: 0.4395\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4036 - val_loss: 0.4352\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3990 - val_loss: 0.4300\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3948 - val_loss: 0.4274\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3909 - val_loss: 0.4243\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3873 - val_loss: 0.4232\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3844 - val_loss: 0.4183\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3815 - val_loss: 0.4172\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3788 - val_loss: 0.4135\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3765 - val_loss: 0.4135\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3743 - val_loss: 0.4089\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3722 - val_loss: 0.4089\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3704 - val_loss: 0.4069\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3684 - val_loss: 0.4052\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3668 - val_loss: 0.4043\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3650 - val_loss: 0.4014\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3636 - val_loss: 0.4005\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3618 - val_loss: 0.3978\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3605 - val_loss: 0.3980\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3591 - val_loss: 0.3967\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3580 - val_loss: 0.3943\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3567 - val_loss: 0.3924\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3554 - val_loss: 0.3943\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3544 - val_loss: 0.3934\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3534 - val_loss: 0.3910\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3522 - val_loss: 0.3918\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3516 - val_loss: 0.3904\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3506 - val_loss: 0.3887\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3497 - val_loss: 0.3880\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3487 - val_loss: 0.3870\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3478 - val_loss: 0.3860\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3471 - val_loss: 0.3879\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3461 - val_loss: 0.3860\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3458 - val_loss: 0.3848\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3450 - val_loss: 0.3826\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3443 - val_loss: 0.3822\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3435 - val_loss: 0.3807\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3430 - val_loss: 0.3841\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3425 - val_loss: 0.3792\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3417 - val_loss: 0.3783\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3413 - val_loss: 0.3805\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3407 - val_loss: 0.3778\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3401 - val_loss: 0.3811\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3397 - val_loss: 0.3768\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3390 - val_loss: 0.3771\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3385 - val_loss: 0.3777\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3379 - val_loss: 0.3751\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3373 - val_loss: 0.3767\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3370 - val_loss: 0.3758\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3366 - val_loss: 0.3736\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3361 - val_loss: 0.3706\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3354 - val_loss: 0.3733\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3352 - val_loss: 0.3747\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3347 - val_loss: 0.3726\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3343 - val_loss: 0.3715\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3339 - val_loss: 0.3741\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3331 - val_loss: 0.3708\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3328 - val_loss: 0.3703\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3322 - val_loss: 0.3690\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3319 - val_loss: 0.3703\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3315 - val_loss: 0.3707\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3311 - val_loss: 0.3678\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3305 - val_loss: 0.3695\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3300 - val_loss: 0.3656\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3297 - val_loss: 0.3694\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3290 - val_loss: 0.3639\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3285 - val_loss: 0.3693\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3284 - val_loss: 0.3652\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3279 - val_loss: 0.3666\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3275 - val_loss: 0.3658\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3269 - val_loss: 0.3637\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3263 - val_loss: 0.3650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3260 - val_loss: 0.3646\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3258 - val_loss: 0.3606\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3252 - val_loss: 0.3637\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3249 - val_loss: 0.3647\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3244 - val_loss: 0.3605\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3242 - val_loss: 0.3622\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3235 - val_loss: 0.3634\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3232 - val_loss: 0.3591\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3228 - val_loss: 0.3599\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3222 - val_loss: 0.3611\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3220 - val_loss: 0.3582\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3215 - val_loss: 0.3628\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3210 - val_loss: 0.3568\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3206 - val_loss: 0.3600\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3200 - val_loss: 0.3584\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3198 - val_loss: 0.3588\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3195 - val_loss: 0.3575\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3190 - val_loss: 0.3590\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3186 - val_loss: 0.3549\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3182 - val_loss: 0.3585\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3177 - val_loss: 0.3552\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3175 - val_loss: 0.3536\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3171 - val_loss: 0.3519\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - ETA: 0s - loss: 0.319 - 1s 48us/sample - loss: 0.3167 - val_loss: 0.3560\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3162 - val_loss: 0.3515\n",
      "5160/5160 [==============================] - 0s 23us/sample - loss: 0.3566\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen otros muchos callbacks en keras.callbacks() (https://keras.io/api/callbacks/).\n",
    "\n",
    "Si se necesita un control extra, se pueden definir callbacks propios. Un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "10752/11610 [==========================>...] - ETA: 0s - loss: 0.3158\n",
      "val/train: 1.12\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3159 - val_loss: 0.3539\n"
     ]
    }
   ],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "        \n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden implementar callbacks en los siguientes puntos: on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), on_batch_end(). Además se pueden usar para evaluación y predicción, por lo que se pueden implementar también en on_test_begin(), ..., on_test_batch_end() si se llaman desde evaluate(), o en on_predict_begin(), ..., on_predict_batch_end() si se llama desde predict()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualización usando TensorBoard</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard es una herramienta de TensorFlow (se instala automáticamente con este) que permite analizar curvas y compararlas durante el entrenamiento.\n",
    "\n",
    "Para usarlo, basta con modificar el programa para que saque como output el data que quieras visualizar usando unos archivos de log binarios llamados \"event files\". Cada binary data guardado es llamado \"summary\". El servidor de TensorBoard monitorizará el directorio log y permitirá visualizar en directo parámetros como las learning curve durante el entrenamiento. En general te interesa apuntar con el servidor TensorBoard a un root log directory, y configurar tu programa para que escriba en distintos subdirectorios cada vez que ejecutes. De esta forma el servidor de TensorBoard te permitirá comparar entre distintas ejecuciones de tu programa sin mezclarlo todo.\n",
    "\n",
    "El código estará comentado porque es un notebook de estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga el server de TensorBoard en Jupyter asi:\n",
    "\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "# Creacion del directorio:\n",
    "\n",
    "# root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "#def get_run_logdir():\n",
    "#    import time\n",
    "#    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "#    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "#run_logdir = get_run_logdir()\n",
    "#run_logdir\n",
    "\n",
    "# Ejemplo de modelo:\n",
    "\n",
    "#keras.backend.clear_session()\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_random_seed(42)\n",
    "\n",
    "#model = keras.models.Sequential([\n",
    "#    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "#    keras.layers.Dense(30, activation=\"relu\"),\n",
    "#    keras.layers.Dense(1)\n",
    "#])    \n",
    "#model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.05))\n",
    "\n",
    "# Uso del callback TensorBoard de Keras:\n",
    "\n",
    "#tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "#history = model.fit(X_train, y_train, epochs=30,\n",
    "#                    validation_data=(X_valid, y_valid),\n",
    "#                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fine-Tuning de hiperparámetros. Número de capas ocultas, de neuronas por capa, learning rate y otros.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fine-Tuning de hiperparámetros</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La flexibilidad de las NN es también su principal problema, porque tiene muchos hiperparámetros para tunear. Por ejemplo, el número de capas, el número de neuronas por capas, el tipo de función de activación, la lógica de iniciación de los pesos y más. En esta sección aprenderemos cómo combinar de forma eficiente los hiperparámetros.\n",
    "\n",
    "La primera opción es hacer una GridSearchCV o RandomizedSearchCV de distintos parámetros y quedarse con aquello que mejor funcione para el validation test. Para esto hay que convertir los modelos Keras en elementos que Sklearn sepa gestionar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 1.1624 - val_loss: 1.4561\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.7169 - val_loss: 1.8601\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.5865 - val_loss: 0.6223\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.5065 - val_loss: 0.5728\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.4722 - val_loss: 0.4848\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4568 - val_loss: 0.5041\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.4406 - val_loss: 0.4606\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4302 - val_loss: 0.4561\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4227 - val_loss: 0.4730\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4157 - val_loss: 0.4514\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.4103 - val_loss: 0.4482\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4055 - val_loss: 0.4323\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.4016 - val_loss: 0.4400\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3975 - val_loss: 0.4282\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3943 - val_loss: 0.4324\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3916 - val_loss: 0.4231\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3893 - val_loss: 0.4256\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3872 - val_loss: 0.4228\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3841 - val_loss: 0.4234\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3819 - val_loss: 0.4183\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3796 - val_loss: 0.4172\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3774 - val_loss: 0.4153\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3756 - val_loss: 0.4117\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3732 - val_loss: 0.4092\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3716 - val_loss: 0.4103\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3699 - val_loss: 0.4048\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3691 - val_loss: 0.4105\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3673 - val_loss: 0.4008\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3655 - val_loss: 0.4069\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3644 - val_loss: 0.4026\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3627 - val_loss: 0.3985\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3623 - val_loss: 0.4028\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3608 - val_loss: 0.3943\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3599 - val_loss: 0.3978\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3590 - val_loss: 0.3923\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3584 - val_loss: 0.3954\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3570 - val_loss: 0.3943\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3565 - val_loss: 0.3969\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3565 - val_loss: 0.3968\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3563 - val_loss: 0.3895\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3552 - val_loss: 0.3936\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3542 - val_loss: 0.3892\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3532 - val_loss: 0.3868\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3524 - val_loss: 0.3867\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3525 - val_loss: 0.3931\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3515 - val_loss: 0.3863\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3509 - val_loss: 0.3864\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3499 - val_loss: 0.3886\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3498 - val_loss: 0.3818\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3486 - val_loss: 0.3836\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3482 - val_loss: 0.3833\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3473 - val_loss: 0.3812\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3469 - val_loss: 0.3803\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3460 - val_loss: 0.3804\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3451 - val_loss: 0.3782\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3447 - val_loss: 0.3831\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3442 - val_loss: 0.3749\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3433 - val_loss: 0.3781\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3425 - val_loss: 0.3763\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3418 - val_loss: 0.3778\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3410 - val_loss: 0.3743\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3404 - val_loss: 0.3758\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3402 - val_loss: 0.3732\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3393 - val_loss: 0.3699\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3385 - val_loss: 0.3736\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3383 - val_loss: 0.3703\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3376 - val_loss: 0.3773\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3374 - val_loss: 0.3686\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3371 - val_loss: 0.3776\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3364 - val_loss: 0.3671\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3355 - val_loss: 0.3706\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3348 - val_loss: 0.3665\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3347 - val_loss: 0.3680\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3343 - val_loss: 0.3675\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3337 - val_loss: 0.3686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3331 - val_loss: 0.3641\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3328 - val_loss: 0.3703\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3330 - val_loss: 0.3634\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3320 - val_loss: 0.3668\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3315 - val_loss: 0.3686\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3313 - val_loss: 0.3618\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3312 - val_loss: 0.3685\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3307 - val_loss: 0.3634\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3300 - val_loss: 0.3615\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3295 - val_loss: 0.3622\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3291 - val_loss: 0.3598\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3291 - val_loss: 0.3605\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3284 - val_loss: 0.3589\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3282 - val_loss: 0.3613\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3277 - val_loss: 0.3588\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3278 - val_loss: 0.3672\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3271 - val_loss: 0.3589\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3268 - val_loss: 0.3592\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3268 - val_loss: 0.3576\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3261 - val_loss: 0.3654\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3267 - val_loss: 0.3551\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3257 - val_loss: 0.3727\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3260 - val_loss: 0.3592\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3254 - val_loss: 0.3566\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 1s 43us/sample - loss: 0.3243 - val_loss: 0.3615\n",
      "5160/5160 [==============================] - 0s 21us/sample - loss: 0.3608\n"
     ]
    }
   ],
   "source": [
    "# Limpiamos de modelos y establecemos semilla\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_random_seed(42)\n",
    "\n",
    "# Primero definimos la función genérica que crea el modelo que vamos a generar. Este modelo será secuencial y \n",
    "# para una regresión univariante (una neurona output):\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu',**options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1, **options))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "# El diccionario options se usa para asegurar que la primera capa es correctamente informada en el input shape, \n",
    "# así si n_hidden=0, la primera capa será la capa output.\n",
    "\n",
    "# Una vez se ha creado el modelo genérico, se crea un KerasRegressor, que ya permite ser usado como un objeto de \n",
    "# Sklearn:\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "# Como no hemos especificado parámetros al crear el keras_reg, va a usar los genéricos que pusimos en la definición\n",
    "# de build_model. Ya podemos usar este elemento como un regressor de Sklearn: entrenarlo con fit(), evaluar con \n",
    "# score() y predecir con predict().\n",
    "\n",
    "# NOTA: cualquier hiperparámetro que se pase por fit() se pasará al modelo Keras inferior, y el score que se \n",
    "# obtendrá será el opuesto del MSE, porque Sklearn busca scores, no losses (más alto es mejor):\n",
    "\n",
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, no queremos entrenar un único modelo como arriba, si no que queremos entrenar cientos de versiones y ver cual tiene mejor performance. Como existen multitud de hiperparámetros, es aconsejable usar RandomizedSearchCV que GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_neurons=78, n_hidden=2, learning_rate=0.0005086711330891658 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 70us/sample - loss: 3.2103 - val_loss: 2.0331\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 1.2700 - val_loss: 1.1257\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.8671 - val_loss: 0.8385\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.7441 - val_loss: 0.7457\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6947 - val_loss: 0.7025\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6664 - val_loss: 0.6779\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6461 - val_loss: 0.6591\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6290 - val_loss: 0.6438\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6135 - val_loss: 0.6309\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6002 - val_loss: 0.6168\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5874 - val_loss: 0.6057\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5757 - val_loss: 0.5950\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5651 - val_loss: 0.5854\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5548 - val_loss: 0.5756\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5456 - val_loss: 0.5670\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5369 - val_loss: 0.5592\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5285 - val_loss: 0.5515\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5209 - val_loss: 0.5443\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5137 - val_loss: 0.5392\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5068 - val_loss: 0.5340\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5003 - val_loss: 0.5263\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4943 - val_loss: 0.5205\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4885 - val_loss: 0.5139\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4830 - val_loss: 0.5083\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4781 - val_loss: 0.5058\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4728 - val_loss: 0.4990\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4683 - val_loss: 0.4951\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4641 - val_loss: 0.4923\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4595 - val_loss: 0.4882\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4554 - val_loss: 0.4844\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4515 - val_loss: 0.4787\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4483 - val_loss: 0.4780\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4445 - val_loss: 0.4741\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4410 - val_loss: 0.4688\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4380 - val_loss: 0.4670\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4348 - val_loss: 0.4643\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4318 - val_loss: 0.4606\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4290 - val_loss: 0.4597\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4262 - val_loss: 0.4561\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4235 - val_loss: 0.4522\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4211 - val_loss: 0.4505\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4186 - val_loss: 0.4480\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4161 - val_loss: 0.4457\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4141 - val_loss: 0.4455\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4116 - val_loss: 0.4429\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4097 - val_loss: 0.4401\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4074 - val_loss: 0.4380\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4054 - val_loss: 0.4366\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4034 - val_loss: 0.4334\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4016 - val_loss: 0.4316\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3998 - val_loss: 0.4303\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3979 - val_loss: 0.4306\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3962 - val_loss: 0.4271\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3947 - val_loss: 0.4265\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3929 - val_loss: 0.4254\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3912 - val_loss: 0.4241\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3896 - val_loss: 0.4226\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3881 - val_loss: 0.4203\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3867 - val_loss: 0.4201\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3853 - val_loss: 0.4195\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3839 - val_loss: 0.4185\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3825 - val_loss: 0.4171\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3811 - val_loss: 0.4141\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3798 - val_loss: 0.4135\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3786 - val_loss: 0.4121\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3774 - val_loss: 0.4107\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3761 - val_loss: 0.4112\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3747 - val_loss: 0.4087\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3741 - val_loss: 0.4085\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3728 - val_loss: 0.4086\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3718 - val_loss: 0.4067\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3705 - val_loss: 0.4055\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3696 - val_loss: 0.4037\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3685 - val_loss: 0.4027\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3677 - val_loss: 0.4022\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3665 - val_loss: 0.4027\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3658 - val_loss: 0.4007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3648 - val_loss: 0.3998\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3639 - val_loss: 0.3989\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3631 - val_loss: 0.3993\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3622 - val_loss: 0.3974\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3614 - val_loss: 0.3968\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3606 - val_loss: 0.3968\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3598 - val_loss: 0.3954\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3590 - val_loss: 0.3947\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3583 - val_loss: 0.3938\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3575 - val_loss: 0.3943\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3567 - val_loss: 0.3931\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3561 - val_loss: 0.3920\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3553 - val_loss: 0.3917\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3545 - val_loss: 0.3913\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3539 - val_loss: 0.3900\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3532 - val_loss: 0.3897\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3526 - val_loss: 0.3893\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3520 - val_loss: 0.3888\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3513 - val_loss: 0.3880\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3507 - val_loss: 0.3872\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3501 - val_loss: 0.3869\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3495 - val_loss: 0.3861\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3488 - val_loss: 0.3854\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3686\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 0.3482\n",
      "[CV]  n_neurons=78, n_hidden=2, learning_rate=0.0005086711330891658, total=  41.6s\n",
      "[CV] n_neurons=78, n_hidden=2, learning_rate=0.0005086711330891658 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 72us/sample - loss: 3.3178 - val_loss: 2.7328\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.4822 - val_loss: 1.3906\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.9168 - val_loss: 0.8955\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.7370 - val_loss: 0.7397\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6728 - val_loss: 0.6830\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6427 - val_loss: 0.6552\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6235 - val_loss: 0.6386\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6088 - val_loss: 0.6265\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5959 - val_loss: 0.6156\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5844 - val_loss: 0.6050\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5738 - val_loss: 0.5949\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5639 - val_loss: 0.5877\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5546 - val_loss: 0.5778\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5461 - val_loss: 0.5687\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5379 - val_loss: 0.5619\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5301 - val_loss: 0.5548\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5228 - val_loss: 0.5477\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5155 - val_loss: 0.5424\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5090 - val_loss: 0.5362\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5020 - val_loss: 0.5307\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4958 - val_loss: 0.5220\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4894 - val_loss: 0.5165\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4830 - val_loss: 0.5099\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4772 - val_loss: 0.5040\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4716 - val_loss: 0.4988\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4665 - val_loss: 0.4952\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4617 - val_loss: 0.4916\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4572 - val_loss: 0.4863\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4531 - val_loss: 0.4828\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4491 - val_loss: 0.4806\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4454 - val_loss: 0.4766\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4418 - val_loss: 0.4730\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4383 - val_loss: 0.4685\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4351 - val_loss: 0.4670\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4322 - val_loss: 0.4644\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4291 - val_loss: 0.4618\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4265 - val_loss: 0.4593\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4237 - val_loss: 0.4567\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4212 - val_loss: 0.4532\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4189 - val_loss: 0.4518\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4167 - val_loss: 0.4507\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4143 - val_loss: 0.4478\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4123 - val_loss: 0.4461\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4102 - val_loss: 0.4444\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4083 - val_loss: 0.4424\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4064 - val_loss: 0.4417\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4046 - val_loss: 0.4406\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4028 - val_loss: 0.4383\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4010 - val_loss: 0.4365\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3994 - val_loss: 0.4359\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3978 - val_loss: 0.4339\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3964 - val_loss: 0.4328\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3948 - val_loss: 0.4313\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3935 - val_loss: 0.4312\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3922 - val_loss: 0.4305\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3908 - val_loss: 0.4273\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3896 - val_loss: 0.4277\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3884 - val_loss: 0.4267\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3870 - val_loss: 0.4253\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3859 - val_loss: 0.4243\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3848 - val_loss: 0.4235\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3837 - val_loss: 0.4224\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3825 - val_loss: 0.4212\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3814 - val_loss: 0.4213\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3804 - val_loss: 0.4201\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3794 - val_loss: 0.4182\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3785 - val_loss: 0.4191\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3775 - val_loss: 0.4187\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3764 - val_loss: 0.4170\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3756 - val_loss: 0.4163\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3747 - val_loss: 0.4156\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3737 - val_loss: 0.4152\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3729 - val_loss: 0.4144\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3721 - val_loss: 0.4133\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3711 - val_loss: 0.4125\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3704 - val_loss: 0.4114\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3696 - val_loss: 0.4113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3689 - val_loss: 0.4106\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3681 - val_loss: 0.4114\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3673 - val_loss: 0.4104\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3666 - val_loss: 0.4092\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3659 - val_loss: 0.4090\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3652 - val_loss: 0.4085\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3645 - val_loss: 0.4085\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3638 - val_loss: 0.4069\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3632 - val_loss: 0.4070\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3625 - val_loss: 0.4063\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3619 - val_loss: 0.4059\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3613 - val_loss: 0.4061\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3607 - val_loss: 0.4044\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3601 - val_loss: 0.4035\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3595 - val_loss: 0.4040\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3589 - val_loss: 0.4042\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3584 - val_loss: 0.4020\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3577 - val_loss: 0.4019\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3571 - val_loss: 0.4009\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3567 - val_loss: 0.4022\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3561 - val_loss: 0.4012\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3556 - val_loss: 0.4002\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3550 - val_loss: 0.4017\n",
      "3870/3870 [==============================] - 0s 22us/sample - loss: 0.3634\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: 0.3545\n",
      "[CV]  n_neurons=78, n_hidden=2, learning_rate=0.0005086711330891658, total=  40.7s\n",
      "[CV] n_neurons=78, n_hidden=2, learning_rate=0.0005086711330891658 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 2.4518 - val_loss: 2.3912\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 1.0856 - val_loss: 1.8498\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.8601 - val_loss: 1.3383\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.7678 - val_loss: 1.0452\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.7139 - val_loss: 0.8763\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6777 - val_loss: 0.7679\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.6504 - val_loss: 0.6969\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6289 - val_loss: 0.6506\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6108 - val_loss: 0.6211\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5953 - val_loss: 0.6012\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5817 - val_loss: 0.5874\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5695 - val_loss: 0.5777\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5585 - val_loss: 0.5701\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5485 - val_loss: 0.5656\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5394 - val_loss: 0.5595\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5309 - val_loss: 0.5549\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5230 - val_loss: 0.5499\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5157 - val_loss: 0.5480\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5091 - val_loss: 0.5420\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5027 - val_loss: 0.5383\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4967 - val_loss: 0.5341\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4910 - val_loss: 0.5298\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4856 - val_loss: 0.5255\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4805 - val_loss: 0.5183\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4758 - val_loss: 0.5139\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4710 - val_loss: 0.5080\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4668 - val_loss: 0.5054\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4626 - val_loss: 0.4995\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4586 - val_loss: 0.4941\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4547 - val_loss: 0.4895\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4511 - val_loss: 0.4847\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4476 - val_loss: 0.4808\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4441 - val_loss: 0.4758\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4409 - val_loss: 0.4719\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4377 - val_loss: 0.4691\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4347 - val_loss: 0.4642\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4318 - val_loss: 0.4611\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4289 - val_loss: 0.4591\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4264 - val_loss: 0.4545\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4237 - val_loss: 0.4517\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4212 - val_loss: 0.4486\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4188 - val_loss: 0.4462\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4164 - val_loss: 0.4434\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4140 - val_loss: 0.4405\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4121 - val_loss: 0.4388\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4098 - val_loss: 0.4362\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4079 - val_loss: 0.4345\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4059 - val_loss: 0.4322\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4039 - val_loss: 0.4303\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4020 - val_loss: 0.4287\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4002 - val_loss: 0.4270\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3984 - val_loss: 0.4254\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3967 - val_loss: 0.4241\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3951 - val_loss: 0.4229\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3936 - val_loss: 0.4217\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3920 - val_loss: 0.4206\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3904 - val_loss: 0.4196\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3890 - val_loss: 0.4188\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3875 - val_loss: 0.4179\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3860 - val_loss: 0.4172\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3848 - val_loss: 0.4162\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3835 - val_loss: 0.4156\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3822 - val_loss: 0.4148\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3810 - val_loss: 0.4142\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3797 - val_loss: 0.4133\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3786 - val_loss: 0.4127\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3775 - val_loss: 0.4118\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3764 - val_loss: 0.4109\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3754 - val_loss: 0.4104\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3744 - val_loss: 0.4105\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3734 - val_loss: 0.4098\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3724 - val_loss: 0.4092\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3713 - val_loss: 0.4098\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3704 - val_loss: 0.4087\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3696 - val_loss: 0.4081\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3688 - val_loss: 0.4081\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3678 - val_loss: 0.4094\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3671 - val_loss: 0.4083\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3662 - val_loss: 0.4078\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3655 - val_loss: 0.4069\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3647 - val_loss: 0.4070\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3639 - val_loss: 0.4069\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3631 - val_loss: 0.4063\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3624 - val_loss: 0.4066\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3617 - val_loss: 0.4061\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3611 - val_loss: 0.4057\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3604 - val_loss: 0.4054\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3597 - val_loss: 0.4050\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3591 - val_loss: 0.4051\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3584 - val_loss: 0.4047\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3579 - val_loss: 0.4042\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3573 - val_loss: 0.4041\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3566 - val_loss: 0.4039\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3562 - val_loss: 0.4047\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3556 - val_loss: 0.4048\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3551 - val_loss: 0.4031\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3546 - val_loss: 0.4029\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3540 - val_loss: 0.4027\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3536 - val_loss: 0.4028\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3529 - val_loss: 0.4031\n",
      "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3518\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 0.3523\n",
      "[CV]  n_neurons=78, n_hidden=2, learning_rate=0.0005086711330891658, total=  41.0s\n",
      "[CV] n_neurons=28, n_hidden=1, learning_rate=0.0234500676981933 ......\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 71us/sample - loss: 2.3407 - val_loss: 6.9341\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6841 - val_loss: 671.4133\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: nan - val_loss: nan\n",
      "3870/3870 [==============================] - 0s 21us/sample - loss: nan\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan\n",
      "[CV]  n_neurons=28, n_hidden=1, learning_rate=0.0234500676981933, total=   4.6s\n",
      "[CV] n_neurons=28, n_hidden=1, learning_rate=0.0234500676981933 ......\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 80us/sample - loss: 0.7407 - val_loss: 0.5457\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4512 - val_loss: 0.4549\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4172 - val_loss: 0.4479\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4043 - val_loss: 0.4335\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.4130 - val_loss: 0.4362\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3862 - val_loss: 0.4164\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3777 - val_loss: 0.4110\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3764 - val_loss: 0.4043\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3723 - val_loss: 0.4005\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3666 - val_loss: 0.4076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3751 - val_loss: 0.3993\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3839 - val_loss: 0.4089\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3773 - val_loss: 0.4053\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3551 - val_loss: 0.3894\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3551 - val_loss: 0.3854\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3510 - val_loss: 0.3823\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3921 - val_loss: 0.4007\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4824 - val_loss: 0.4232\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3668 - val_loss: 0.4055\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3544 - val_loss: 0.3896\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3492 - val_loss: 0.3918\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3530 - val_loss: 0.3803\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3452 - val_loss: 0.3872\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3566 - val_loss: 0.3786\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3387 - val_loss: 0.3789\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3427 - val_loss: 0.3785\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3353 - val_loss: 0.3857\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3335 - val_loss: 0.3677\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3320 - val_loss: 0.3801\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3304 - val_loss: 0.3678\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3291 - val_loss: 0.3660\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3280 - val_loss: 0.3655\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3253 - val_loss: 0.3642\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3227 - val_loss: 0.3633\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3222 - val_loss: 0.3591\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3201 - val_loss: 0.3631\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3261 - val_loss: 0.3651\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3219 - val_loss: 0.3654\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3188 - val_loss: 0.3618\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3190 - val_loss: 0.3628\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3190 - val_loss: 0.3774\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3156 - val_loss: 0.3601\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3277 - val_loss: 0.3621\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3179 - val_loss: 0.3560\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3143 - val_loss: 0.3604\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3145 - val_loss: 0.3490\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3124 - val_loss: 0.3502\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3119 - val_loss: 0.3740\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3106 - val_loss: 0.3424\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3104 - val_loss: 0.3646\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3087 - val_loss: 0.3584\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3114 - val_loss: 0.3488\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3100 - val_loss: 0.3653\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3059 - val_loss: 0.3517\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3052 - val_loss: 0.3491\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3074 - val_loss: 0.3419\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3119 - val_loss: 0.3512\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3047 - val_loss: 0.3455\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3150 - val_loss: 0.3663\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3119 - val_loss: 0.3528\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3086 - val_loss: 0.3626\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3040 - val_loss: 0.3460\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3022 - val_loss: 0.3704\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3034 - val_loss: 0.3395\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3020 - val_loss: 0.3366\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.2996 - val_loss: 0.3548\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.2990 - val_loss: 0.3434\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2993 - val_loss: 0.3442\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2987 - val_loss: 0.3549\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3030 - val_loss: 0.3444\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.2986 - val_loss: 0.3420\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2968 - val_loss: 0.3470\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2962 - val_loss: 0.3445\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.2959 - val_loss: 0.3368\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3165 - val_loss: 0.3449\n",
      "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3166\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: 0.2962\n",
      "[CV]  n_neurons=28, n_hidden=1, learning_rate=0.0234500676981933, total=  29.4s\n",
      "[CV] n_neurons=28, n_hidden=1, learning_rate=0.0234500676981933 ......\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 78us/sample - loss: 0.6556 - val_loss: 0.7189\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4916 - val_loss: 9.0666\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6288 - val_loss: 2.0078\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4474 - val_loss: 0.6320\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4032 - val_loss: 0.4464\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3919 - val_loss: 0.4268\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3876 - val_loss: 0.4173\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3826 - val_loss: 0.4171\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3782 - val_loss: 0.4179\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3737 - val_loss: 0.4092\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3697 - val_loss: 0.4040\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3673 - val_loss: 0.4107\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3657 - val_loss: 0.3979\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3622 - val_loss: 0.3958\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3601 - val_loss: 0.3904\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3567 - val_loss: 0.3974\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3547 - val_loss: 0.3918\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3521 - val_loss: 0.3912\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3520 - val_loss: 0.3857\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3500 - val_loss: 0.3762\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3484 - val_loss: 0.3879\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3469 - val_loss: 0.3749\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3450 - val_loss: 0.3794\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3440 - val_loss: 0.3839\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3410 - val_loss: 0.3724\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3401 - val_loss: 0.3709\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3375 - val_loss: 0.3735\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3379 - val_loss: 0.3684\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3356 - val_loss: 0.3692\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3345 - val_loss: 0.3616\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3334 - val_loss: 0.3865\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3313 - val_loss: 0.3632\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3305 - val_loss: 0.4318\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3306 - val_loss: 0.4301\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3314 - val_loss: 0.5312\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3317 - val_loss: 0.5277\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3319 - val_loss: 0.7893\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3329 - val_loss: 0.5494\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3271 - val_loss: 0.4851\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3246 - val_loss: 0.3970\n",
      "3870/3870 [==============================] - 0s 21us/sample - loss: 0.3371\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: 0.3209\n",
      "[CV]  n_neurons=28, n_hidden=1, learning_rate=0.0234500676981933, total=  15.7s\n",
      "[CV] n_neurons=28, n_hidden=3, learning_rate=0.001951249119995183 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 91us/sample - loss: 1.6370 - val_loss: 1.2539\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.7140 - val_loss: 0.7293\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5653 - val_loss: 0.5674\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5116 - val_loss: 0.5196\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4763 - val_loss: 0.4959\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4491 - val_loss: 0.4731\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4292 - val_loss: 0.4607\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4137 - val_loss: 0.4487\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4015 - val_loss: 0.4362\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3913 - val_loss: 0.4253\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3837 - val_loss: 0.4208\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3769 - val_loss: 0.4159\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.3712 - val_loss: 0.4111\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3664 - val_loss: 0.4043\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3618 - val_loss: 0.4090\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3586 - val_loss: 0.4013\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3562 - val_loss: 0.3996\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3533 - val_loss: 0.3980\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3510 - val_loss: 0.3982\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3488 - val_loss: 0.3991\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3468 - val_loss: 0.3911\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3452 - val_loss: 0.3925\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3441 - val_loss: 0.3934\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3424 - val_loss: 0.3868\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3412 - val_loss: 0.3908\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3399 - val_loss: 0.3854\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3391 - val_loss: 0.3861\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3377 - val_loss: 0.3853\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3367 - val_loss: 0.3809\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3355 - val_loss: 0.3880\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3346 - val_loss: 0.3788\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3337 - val_loss: 0.3797\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3326 - val_loss: 0.3803\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3315 - val_loss: 0.3799\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3308 - val_loss: 0.3768\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3293 - val_loss: 0.3772\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3288 - val_loss: 0.3781\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3282 - val_loss: 0.3783\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3273 - val_loss: 0.3745\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3262 - val_loss: 0.3750\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3256 - val_loss: 0.3754\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3250 - val_loss: 0.3699\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3238 - val_loss: 0.3710\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3234 - val_loss: 0.3736\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3223 - val_loss: 0.3727\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3217 - val_loss: 0.3756\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3210 - val_loss: 0.3680\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3201 - val_loss: 0.3695\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3194 - val_loss: 0.3661\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3184 - val_loss: 0.3664\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3173 - val_loss: 0.3676\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3166 - val_loss: 0.3685\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3161 - val_loss: 0.3676\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3153 - val_loss: 0.3629\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3145 - val_loss: 0.3634\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3137 - val_loss: 0.3609\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3124 - val_loss: 0.3628\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3122 - val_loss: 0.3604\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3107 - val_loss: 0.3604\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3109 - val_loss: 0.3594\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3098 - val_loss: 0.3577\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3087 - val_loss: 0.3590\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3085 - val_loss: 0.3593\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3076 - val_loss: 0.3580\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3064 - val_loss: 0.3589\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3061 - val_loss: 0.3580\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3052 - val_loss: 0.3569\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3046 - val_loss: 0.3597\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3044 - val_loss: 0.3560\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3033 - val_loss: 0.3559\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3031 - val_loss: 0.3584\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3022 - val_loss: 0.3536\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3019 - val_loss: 0.3549\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3006 - val_loss: 0.3528\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3004 - val_loss: 0.3520\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2997 - val_loss: 0.3511\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2987 - val_loss: 0.3527\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2990 - val_loss: 0.3500\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2984 - val_loss: 0.3497\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2980 - val_loss: 0.3499\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2967 - val_loss: 0.3496\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2964 - val_loss: 0.3487\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2961 - val_loss: 0.3475\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2953 - val_loss: 0.3462\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2949 - val_loss: 0.3464\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2938 - val_loss: 0.3482\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2940 - val_loss: 0.3514\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2933 - val_loss: 0.3458\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2927 - val_loss: 0.3466\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2914 - val_loss: 0.3475\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2915 - val_loss: 0.3446\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2912 - val_loss: 0.3451\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2907 - val_loss: 0.3421\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2901 - val_loss: 0.3432\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2890 - val_loss: 0.3397\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2887 - val_loss: 0.3422\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2888 - val_loss: 0.3431\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2883 - val_loss: 0.3413\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2879 - val_loss: 0.3426\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2870 - val_loss: 0.3414\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3129\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 0.2855\n",
      "[CV]  n_neurons=28, n_hidden=3, learning_rate=0.001951249119995183, total=  44.3s\n",
      "[CV] n_neurons=28, n_hidden=3, learning_rate=0.001951249119995183 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 89us/sample - loss: 2.1205 - val_loss: 0.9561\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.7756 - val_loss: 0.7004\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.6523 - val_loss: 0.6536\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6115 - val_loss: 0.6202\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5802 - val_loss: 0.5928\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5513 - val_loss: 0.5650\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5245 - val_loss: 0.5409\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5000 - val_loss: 0.5164\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4779 - val_loss: 0.4967\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4571 - val_loss: 0.4831\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4408 - val_loss: 0.4663\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4265 - val_loss: 0.4572\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4149 - val_loss: 0.4463\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4059 - val_loss: 0.4344\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3982 - val_loss: 0.4260\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3916 - val_loss: 0.4225\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3858 - val_loss: 0.4222\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3815 - val_loss: 0.4156\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3767 - val_loss: 0.4164\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3735 - val_loss: 0.4078\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3697 - val_loss: 0.4075\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3664 - val_loss: 0.4038\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3636 - val_loss: 0.4050\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3613 - val_loss: 0.3988\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3583 - val_loss: 0.3975\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3560 - val_loss: 0.3944\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3536 - val_loss: 0.3904\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3509 - val_loss: 0.3900\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3493 - val_loss: 0.3884\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3474 - val_loss: 0.3849\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3458 - val_loss: 0.3881\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3436 - val_loss: 0.3812\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3423 - val_loss: 0.3801\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3402 - val_loss: 0.3806\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3390 - val_loss: 0.3775\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3375 - val_loss: 0.3766\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3359 - val_loss: 0.3779\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3346 - val_loss: 0.3739\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3336 - val_loss: 0.3755\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3322 - val_loss: 0.3721\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3307 - val_loss: 0.3717\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3289 - val_loss: 0.3725\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3280 - val_loss: 0.3672\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3271 - val_loss: 0.3676\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3250 - val_loss: 0.3688\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3251 - val_loss: 0.3650\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3239 - val_loss: 0.3657\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3229 - val_loss: 0.3654\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3217 - val_loss: 0.3623\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3204 - val_loss: 0.3627\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3198 - val_loss: 0.3653\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3183 - val_loss: 0.3581\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3181 - val_loss: 0.3599\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3170 - val_loss: 0.3559\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3164 - val_loss: 0.3583\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3153 - val_loss: 0.3572\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3149 - val_loss: 0.3542\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3144 - val_loss: 0.3571\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 1s 71us/sample - loss: 0.3133 - val_loss: 0.3611\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3130 - val_loss: 0.3508\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3120 - val_loss: 0.3550\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3118 - val_loss: 0.3522\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3107 - val_loss: 0.3523\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3096 - val_loss: 0.3511\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3090 - val_loss: 0.3542\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: 0.3087 - val_loss: 0.3499\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3081 - val_loss: 0.3486\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3077 - val_loss: 0.3483\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3066 - val_loss: 0.3548\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3058 - val_loss: 0.3534\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3057 - val_loss: 0.3514\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3043 - val_loss: 0.3548\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3045 - val_loss: 0.3487\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3036 - val_loss: 0.3449\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3027 - val_loss: 0.3470\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3026 - val_loss: 0.3432\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3018 - val_loss: 0.3454\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3016 - val_loss: 0.3457\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3010 - val_loss: 0.3462\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3003 - val_loss: 0.3462\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2996 - val_loss: 0.3461\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2988 - val_loss: 0.3409\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2985 - val_loss: 0.3495\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2978 - val_loss: 0.3410\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2972 - val_loss: 0.3444\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2968 - val_loss: 0.3446\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2958 - val_loss: 0.3428\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2956 - val_loss: 0.3395\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2953 - val_loss: 0.3384\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2946 - val_loss: 0.3507\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2943 - val_loss: 0.3371\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2940 - val_loss: 0.3390\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2933 - val_loss: 0.3378\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2928 - val_loss: 0.3419\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2924 - val_loss: 0.3365\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2917 - val_loss: 0.3453\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2920 - val_loss: 0.3335\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2915 - val_loss: 0.3426\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2908 - val_loss: 0.3366\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2902 - val_loss: 0.3382\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3044\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 0.2875\n",
      "[CV]  n_neurons=28, n_hidden=3, learning_rate=0.001951249119995183, total=  45.6s\n",
      "[CV] n_neurons=28, n_hidden=3, learning_rate=0.001951249119995183 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 93us/sample - loss: 1.6363 - val_loss: 1.1095\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.6759 - val_loss: 0.7295\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5970 - val_loss: 0.6078\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5574 - val_loss: 0.5579\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5275 - val_loss: 0.5300\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5038 - val_loss: 0.5088\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4834 - val_loss: 0.4932\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4657 - val_loss: 0.4812\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4507 - val_loss: 0.4673\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4382 - val_loss: 0.4643\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4275 - val_loss: 0.4552\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4183 - val_loss: 0.4518\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4102 - val_loss: 0.4390\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4030 - val_loss: 0.4305\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3970 - val_loss: 0.4225\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3910 - val_loss: 0.4134\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3857 - val_loss: 0.4129\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3812 - val_loss: 0.4063\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3774 - val_loss: 0.4008\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3737 - val_loss: 0.3970\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3704 - val_loss: 0.3947\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3671 - val_loss: 0.3930\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.3646 - val_loss: 0.3891\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3621 - val_loss: 0.3868\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3597 - val_loss: 0.3849\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3576 - val_loss: 0.3862\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3555 - val_loss: 0.3822\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3533 - val_loss: 0.3818\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3515 - val_loss: 0.3825\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 0.3487 - val_loss: 0.3820\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3480 - val_loss: 0.3776\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3459 - val_loss: 0.3768\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3443 - val_loss: 0.3770\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3423 - val_loss: 0.3807\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3413 - val_loss: 0.3770\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3399 - val_loss: 0.3719\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3380 - val_loss: 0.3727\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3367 - val_loss: 0.3724\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3357 - val_loss: 0.3741\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3341 - val_loss: 0.3664\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3332 - val_loss: 0.3695\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3316 - val_loss: 0.3695\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3303 - val_loss: 0.3702\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3296 - val_loss: 0.3662\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3284 - val_loss: 0.3643\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3269 - val_loss: 0.3677\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3262 - val_loss: 0.3657\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3252 - val_loss: 0.3623\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3245 - val_loss: 0.3615\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3226 - val_loss: 0.3658\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3222 - val_loss: 0.3632\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3208 - val_loss: 0.3592\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3200 - val_loss: 0.3569\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3189 - val_loss: 0.3597\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3183 - val_loss: 0.3559\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3168 - val_loss: 0.3661\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3164 - val_loss: 0.3588\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3154 - val_loss: 0.3563\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3151 - val_loss: 0.3537\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3139 - val_loss: 0.3520\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3129 - val_loss: 0.3559\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3123 - val_loss: 0.3527\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3114 - val_loss: 0.3550\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3108 - val_loss: 0.3553\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3097 - val_loss: 0.3547\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3095 - val_loss: 0.3508\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3084 - val_loss: 0.3526\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3080 - val_loss: 0.3451\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3075 - val_loss: 0.3488\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3066 - val_loss: 0.3471\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3055 - val_loss: 0.3555\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3054 - val_loss: 0.3471\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3044 - val_loss: 0.3519\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3039 - val_loss: 0.3473\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3029 - val_loss: 0.3519\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3031 - val_loss: 0.3480\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3025 - val_loss: 0.3436\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3012 - val_loss: 0.3419\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3010 - val_loss: 0.3441\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3007 - val_loss: 0.3384\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2997 - val_loss: 0.3534\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2989 - val_loss: 0.3426\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2988 - val_loss: 0.3448\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2986 - val_loss: 0.3462\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2977 - val_loss: 0.3367\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2974 - val_loss: 0.3439\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2970 - val_loss: 0.3444\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2964 - val_loss: 0.3445\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2965 - val_loss: 0.3412\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2954 - val_loss: 0.3441\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2950 - val_loss: 0.3438\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2944 - val_loss: 0.3472\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2942 - val_loss: 0.3452\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2941 - val_loss: 0.3428\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2929 - val_loss: 0.3465\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.3021\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 0.2914\n",
      "[CV]  n_neurons=28, n_hidden=3, learning_rate=0.001951249119995183, total=  42.5s\n",
      "[CV] n_neurons=16, n_hidden=1, learning_rate=0.005968408643972776 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 85us/sample - loss: 1.3470 - val_loss: 46.6442\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 3.8791 - val_loss: 0.7782\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6022 - val_loss: 0.5326\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4793 - val_loss: 0.4875\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4433 - val_loss: 0.4648\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4211 - val_loss: 0.4489\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4104 - val_loss: 0.4410\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3992 - val_loss: 0.4312\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3956 - val_loss: 0.4419\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3946 - val_loss: 0.4305\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3878 - val_loss: 0.4256\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3840 - val_loss: 0.4203\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3824 - val_loss: 0.4201\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3806 - val_loss: 0.4189\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3781 - val_loss: 0.4164\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3763 - val_loss: 0.4135\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3757 - val_loss: 0.4158\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3771 - val_loss: 0.4152\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3757 - val_loss: 0.4127\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3748 - val_loss: 0.4149\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3736 - val_loss: 0.4132\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3714 - val_loss: 0.4104\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3712 - val_loss: 0.4098\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3706 - val_loss: 0.4079\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3705 - val_loss: 0.4087\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3709 - val_loss: 0.4072\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.3683 - val_loss: 0.4103\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3688 - val_loss: 0.4115\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3677 - val_loss: 0.4072\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3672 - val_loss: 0.4042\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3666 - val_loss: 0.4049\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3658 - val_loss: 0.4035\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3647 - val_loss: 0.4026\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3648 - val_loss: 0.4051\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3638 - val_loss: 0.4041\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3645 - val_loss: 0.4049\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3625 - val_loss: 0.4018\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3622 - val_loss: 0.4018\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3609 - val_loss: 0.4019\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3595 - val_loss: 0.4028\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3605 - val_loss: 0.4008\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3587 - val_loss: 0.3979\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3584 - val_loss: 0.3975\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3583 - val_loss: 0.3980\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3574 - val_loss: 0.3970\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3572 - val_loss: 0.3955\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3546 - val_loss: 0.4002\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3552 - val_loss: 0.3947\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3531 - val_loss: 0.3961\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3546 - val_loss: 0.3974\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3527 - val_loss: 0.3932\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3524 - val_loss: 0.3927\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3514 - val_loss: 0.3930\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3517 - val_loss: 0.3928\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3509 - val_loss: 0.3931\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3499 - val_loss: 0.3943\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3490 - val_loss: 0.3898\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3476 - val_loss: 0.3913\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3481 - val_loss: 0.3929\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3478 - val_loss: 0.3878\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3474 - val_loss: 0.3902\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3464 - val_loss: 0.3877\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3465 - val_loss: 0.3915\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3456 - val_loss: 0.3874\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3465 - val_loss: 0.3870\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3451 - val_loss: 0.3882\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3444 - val_loss: 0.3863\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3439 - val_loss: 0.3855\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3436 - val_loss: 0.3846\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3443 - val_loss: 0.3864\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3427 - val_loss: 0.3890\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3437 - val_loss: 0.3840\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3425 - val_loss: 0.3829\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3417 - val_loss: 0.3867\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3411 - val_loss: 0.3856\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3410 - val_loss: 0.3832\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3401 - val_loss: 0.3856\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3396 - val_loss: 0.3813\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3398 - val_loss: 0.3810\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3391 - val_loss: 0.3801\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3386 - val_loss: 0.3797\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3382 - val_loss: 0.3788\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3374 - val_loss: 0.3868\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3387 - val_loss: 0.3813\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3371 - val_loss: 0.3784\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3373 - val_loss: 0.3778\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3357 - val_loss: 0.3783\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3351 - val_loss: 0.3772\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3347 - val_loss: 0.3765\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3339 - val_loss: 0.3801\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3345 - val_loss: 0.3759\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3336 - val_loss: 0.3781\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3334 - val_loss: 0.3755\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3330 - val_loss: 0.3761\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3325 - val_loss: 0.3750\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3315 - val_loss: 0.3755\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3311 - val_loss: 0.3761\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3313 - val_loss: 0.3708\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3296 - val_loss: 0.3722\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3297 - val_loss: 0.3692\n",
      "3870/3870 [==============================] - 0s 21us/sample - loss: 0.3511\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: 0.3273\n",
      "[CV]  n_neurons=16, n_hidden=1, learning_rate=0.005968408643972776, total=  40.3s\n",
      "[CV] n_neurons=16, n_hidden=1, learning_rate=0.005968408643972776 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 88us/sample - loss: 0.9819 - val_loss: 0.6243\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5767 - val_loss: 0.5432\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5080 - val_loss: 0.5081\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4770 - val_loss: 0.4873\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4530 - val_loss: 0.4799\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4383 - val_loss: 0.4612\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4316 - val_loss: 0.4667\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4268 - val_loss: 0.4531\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4211 - val_loss: 0.4510\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4239 - val_loss: 0.4575\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4147 - val_loss: 0.4545\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4097 - val_loss: 0.4425\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4060 - val_loss: 0.4421\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4036 - val_loss: 0.4390\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4003 - val_loss: 0.4371\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3971 - val_loss: 0.4344\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3942 - val_loss: 0.4312\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3923 - val_loss: 0.4344\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3896 - val_loss: 0.4273\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3878 - val_loss: 0.4293\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3851 - val_loss: 0.4222\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3840 - val_loss: 0.4280\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3827 - val_loss: 0.4215\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3796 - val_loss: 0.4225\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3771 - val_loss: 0.4227\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3764 - val_loss: 0.4156\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3784 - val_loss: 0.4236\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3778 - val_loss: 0.4197\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3870 - val_loss: 0.4192\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3717 - val_loss: 0.4176\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3777 - val_loss: 0.4142\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3719 - val_loss: 0.4218\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3716 - val_loss: 0.4214\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3680 - val_loss: 0.4073\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3641 - val_loss: 0.4061\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3641 - val_loss: 0.4092\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3616 - val_loss: 0.4069\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3588 - val_loss: 0.4010\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3617 - val_loss: 0.4052\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3654 - val_loss: 0.4304\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3640 - val_loss: 0.4024\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3550 - val_loss: 0.3989\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3532 - val_loss: 0.4006\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3516 - val_loss: 0.4080\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3542 - val_loss: 0.4006\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3556 - val_loss: 0.3963\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3479 - val_loss: 0.3968\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3528 - val_loss: 0.3907\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3541 - val_loss: 0.3954\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3500 - val_loss: 0.3937\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3455 - val_loss: 0.3899\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3476 - val_loss: 0.3871\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3465 - val_loss: 0.3886\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3456 - val_loss: 0.3912\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3754 - val_loss: 0.3857\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3491 - val_loss: 0.4073\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3507 - val_loss: 0.3903\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3463 - val_loss: 0.3915\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3405 - val_loss: 0.3878\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3458 - val_loss: 0.3880\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3394 - val_loss: 0.3822\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3362 - val_loss: 0.3907\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3382 - val_loss: 0.3830\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3423 - val_loss: 0.3831\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3436 - val_loss: 0.3873\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3355 - val_loss: 0.3746\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3341 - val_loss: 0.3797\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3338 - val_loss: 0.3814\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3334 - val_loss: 0.3808\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3304 - val_loss: 0.3879\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3375 - val_loss: 0.3726\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3330 - val_loss: 0.3821\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3357 - val_loss: 0.3875\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.3502 - val_loss: 0.3746\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3306 - val_loss: 0.3818\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3322 - val_loss: 0.3720\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3339 - val_loss: 0.3846\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3926 - val_loss: 0.3877\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3323 - val_loss: 0.3760\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3292 - val_loss: 0.3816\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3272 - val_loss: 0.3741\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3284 - val_loss: 0.3714\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3288 - val_loss: 0.3706\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.3277 - val_loss: 0.3746\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3250 - val_loss: 0.3658\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3262 - val_loss: 0.3820\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3260 - val_loss: 0.3758\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3301 - val_loss: 0.3738\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3239 - val_loss: 0.3773\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3418 - val_loss: 0.3819\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3318 - val_loss: 0.3870\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3319 - val_loss: 0.3821\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3425 - val_loss: 0.3725\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3408 - val_loss: 0.3767\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3481 - val_loss: 0.3820\n",
      "3870/3870 [==============================] - 0s 22us/sample - loss: 0.3295\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 0.3212\n",
      "[CV]  n_neurons=16, n_hidden=1, learning_rate=0.005968408643972776, total=  37.1s\n",
      "[CV] n_neurons=16, n_hidden=1, learning_rate=0.005968408643972776 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 90us/sample - loss: 1.2735 - val_loss: 0.6006\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.5280 - val_loss: 0.7416\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.5072 - val_loss: 0.5872\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4834 - val_loss: 0.5106\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4645 - val_loss: 0.4952\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4500 - val_loss: 0.4693\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4382 - val_loss: 0.4534\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4292 - val_loss: 0.4463\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4212 - val_loss: 0.4366\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4143 - val_loss: 0.4337\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.4087 - val_loss: 0.4309\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.4041 - val_loss: 0.4301\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.4000 - val_loss: 0.4327\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3966 - val_loss: 0.4292\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3927 - val_loss: 0.4299\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3896 - val_loss: 0.4234\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3877 - val_loss: 0.4324\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3862 - val_loss: 0.4267\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3842 - val_loss: 0.4318\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3822 - val_loss: 0.4351\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3812 - val_loss: 0.4350\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3791 - val_loss: 0.4389\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3786 - val_loss: 0.4318\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3771 - val_loss: 0.4409\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.3756 - val_loss: 0.4433\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.3751 - val_loss: 0.4359\n",
      "3870/3870 [==============================] - 0s 22us/sample - loss: 0.3640\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: 0.3732\n",
      "[CV]  n_neurons=16, n_hidden=1, learning_rate=0.005968408643972776, total=  10.8s\n",
      "[CV] n_neurons=25, n_hidden=2, learning_rate=0.0028297426043239157 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 98us/sample - loss: 1.9037 - val_loss: 3.2618\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8619 - val_loss: 0.6205\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5540 - val_loss: 0.5626\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.5065 - val_loss: 0.5245\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4715 - val_loss: 0.4959\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4463 - val_loss: 0.4787\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4242 - val_loss: 0.4586\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4081 - val_loss: 0.4487\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3944 - val_loss: 0.4360\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3844 - val_loss: 0.4289\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3767 - val_loss: 0.4251\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3708 - val_loss: 0.4191\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3658 - val_loss: 0.4169\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3621 - val_loss: 0.4169\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3590 - val_loss: 0.4104\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3565 - val_loss: 0.4093\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3540 - val_loss: 0.4065\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3522 - val_loss: 0.4062\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3502 - val_loss: 0.4041\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3489 - val_loss: 0.4037\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3474 - val_loss: 0.4044\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3460 - val_loss: 0.4005\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3446 - val_loss: 0.3997\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3434 - val_loss: 0.3992\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3421 - val_loss: 0.3968\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3403 - val_loss: 0.3957\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3403 - val_loss: 0.3948\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3384 - val_loss: 0.3927\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3373 - val_loss: 0.3907\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3361 - val_loss: 0.3891\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3351 - val_loss: 0.3894\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3340 - val_loss: 0.3896\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3335 - val_loss: 0.3864\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3320 - val_loss: 0.3906\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3306 - val_loss: 0.3846\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3298 - val_loss: 0.3850\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3292 - val_loss: 0.3843\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3282 - val_loss: 0.3827\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3272 - val_loss: 0.3840\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3270 - val_loss: 0.3797\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3261 - val_loss: 0.3804\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3253 - val_loss: 0.3804\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3248 - val_loss: 0.3775\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3238 - val_loss: 0.3768\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3228 - val_loss: 0.3765\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3218 - val_loss: 0.3740\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3213 - val_loss: 0.3733\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3206 - val_loss: 0.3744\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3198 - val_loss: 0.3742\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3192 - val_loss: 0.3739\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3185 - val_loss: 0.3719\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3173 - val_loss: 0.3716\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3167 - val_loss: 0.3710\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3159 - val_loss: 0.3691\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3157 - val_loss: 0.3690\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3147 - val_loss: 0.3682\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3142 - val_loss: 0.3707\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3135 - val_loss: 0.3677\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3127 - val_loss: 0.3721\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3124 - val_loss: 0.3668\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3119 - val_loss: 0.3661\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3112 - val_loss: 0.3667\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3104 - val_loss: 0.3644\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3097 - val_loss: 0.3645\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3097 - val_loss: 0.3666\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3088 - val_loss: 0.3665\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3083 - val_loss: 0.3645\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3082 - val_loss: 0.3644\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3072 - val_loss: 0.3631\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3069 - val_loss: 0.3630\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3074 - val_loss: 0.3615\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3057 - val_loss: 0.3599\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3055 - val_loss: 0.3604\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3051 - val_loss: 0.3593\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3040 - val_loss: 0.3603\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3035 - val_loss: 0.3602\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3031 - val_loss: 0.3576\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3029 - val_loss: 0.3599\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3019 - val_loss: 0.3571\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3022 - val_loss: 0.3590\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3013 - val_loss: 0.3554\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3009 - val_loss: 0.3552\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3004 - val_loss: 0.3569\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3001 - val_loss: 0.3605\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2994 - val_loss: 0.3550\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2986 - val_loss: 0.3571\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2988 - val_loss: 0.3573\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2983 - val_loss: 0.3553\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2980 - val_loss: 0.3555\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2967 - val_loss: 0.3549\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2974 - val_loss: 0.3536\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2963 - val_loss: 0.3539\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2965 - val_loss: 0.3524\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2952 - val_loss: 0.3525\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2959 - val_loss: 0.3524\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2949 - val_loss: 0.3538\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2939 - val_loss: 0.3539\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2929 - val_loss: 0.3583\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2940 - val_loss: 0.3516\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2923 - val_loss: 0.3531\n",
      "3870/3870 [==============================] - 0s 23us/sample - loss: 0.3192\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 0.2920\n",
      "[CV]  n_neurons=25, n_hidden=2, learning_rate=0.0028297426043239157, total=  42.2s\n",
      "[CV] n_neurons=25, n_hidden=2, learning_rate=0.0028297426043239157 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 100us/sample - loss: 1.5421 - val_loss: 1.0593\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.7142 - val_loss: 0.6706\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.6023 - val_loss: 0.6134\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5516 - val_loss: 0.5755\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.5130 - val_loss: 0.5392\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4827 - val_loss: 0.5080\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4584 - val_loss: 0.4878\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4397 - val_loss: 0.4722\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4241 - val_loss: 0.4597\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4126 - val_loss: 0.4477\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4029 - val_loss: 0.4368\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3946 - val_loss: 0.4339\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3879 - val_loss: 0.4246\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3823 - val_loss: 0.4233\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3779 - val_loss: 0.4159\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3739 - val_loss: 0.4203\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3709 - val_loss: 0.4136\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3680 - val_loss: 0.4113\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3658 - val_loss: 0.4075\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3633 - val_loss: 0.4052\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3609 - val_loss: 0.4000\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3590 - val_loss: 0.3993\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3571 - val_loss: 0.4009\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3554 - val_loss: 0.3973\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3539 - val_loss: 0.3944\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3526 - val_loss: 0.3950\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3514 - val_loss: 0.3930\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3491 - val_loss: 0.3951\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3487 - val_loss: 0.3871\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3471 - val_loss: 0.3834\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3456 - val_loss: 0.3870\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3447 - val_loss: 0.3853\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3429 - val_loss: 0.3817\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3424 - val_loss: 0.3816\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3418 - val_loss: 0.3852\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3411 - val_loss: 0.3812\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3393 - val_loss: 0.3817\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3382 - val_loss: 0.3802\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3371 - val_loss: 0.3772\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3363 - val_loss: 0.3754\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3352 - val_loss: 0.3832\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3346 - val_loss: 0.3757\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3340 - val_loss: 0.3747\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3335 - val_loss: 0.3761\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3324 - val_loss: 0.3734\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3311 - val_loss: 0.3736\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3299 - val_loss: 0.3764\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3295 - val_loss: 0.3758\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3286 - val_loss: 0.3741\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3292 - val_loss: 0.3706\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3278 - val_loss: 0.3691\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3284 - val_loss: 0.3723\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3270 - val_loss: 0.3668\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3257 - val_loss: 0.3687\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3259 - val_loss: 0.3696\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3248 - val_loss: 0.3706\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3243 - val_loss: 0.3677\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3237 - val_loss: 0.3638\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3231 - val_loss: 0.3726\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3225 - val_loss: 0.3639\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3220 - val_loss: 0.3750\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3225 - val_loss: 0.3611\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3217 - val_loss: 0.3668\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3204 - val_loss: 0.3620\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3202 - val_loss: 0.3701\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3199 - val_loss: 0.3660\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3188 - val_loss: 0.3630\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3185 - val_loss: 0.3596\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3173 - val_loss: 0.3666\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3176 - val_loss: 0.3620\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3171 - val_loss: 0.3560\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3175 - val_loss: 0.3617\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3178 - val_loss: 0.3567\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3160 - val_loss: 0.3638\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3149 - val_loss: 0.3589\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3146 - val_loss: 0.3607\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3138 - val_loss: 0.3571\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3131 - val_loss: 0.3617\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3126 - val_loss: 0.3594\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.3129 - val_loss: 0.3595\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3127 - val_loss: 0.3601\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.3301\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: 0.3143\n",
      "[CV]  n_neurons=25, n_hidden=2, learning_rate=0.0028297426043239157, total=  34.4s\n",
      "[CV] n_neurons=25, n_hidden=2, learning_rate=0.0028297426043239157 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 108us/sample - loss: 1.3462 - val_loss: 0.6339\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.5799 - val_loss: 0.6174\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5384 - val_loss: 0.5979\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5115 - val_loss: 0.5478\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4918 - val_loss: 0.5412\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4788 - val_loss: 0.5229\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4670 - val_loss: 0.5301\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4589 - val_loss: 0.4949\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4510 - val_loss: 0.5048\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4435 - val_loss: 0.5039\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4367 - val_loss: 0.4985\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4318 - val_loss: 0.4694\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4261 - val_loss: 0.4571\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4219 - val_loss: 0.4445\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4166 - val_loss: 0.4392\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4124 - val_loss: 0.4311\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4077 - val_loss: 0.4278\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.4038 - val_loss: 0.4244\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3998 - val_loss: 0.4172\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3957 - val_loss: 0.4148\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3916 - val_loss: 0.4114\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3893 - val_loss: 0.4065\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3860 - val_loss: 0.4043\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3826 - val_loss: 0.4020\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3799 - val_loss: 0.4012\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3772 - val_loss: 0.3976\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3746 - val_loss: 0.3974\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3718 - val_loss: 0.3966\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3696 - val_loss: 0.3952\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3670 - val_loss: 0.3946\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3651 - val_loss: 0.3931\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3627 - val_loss: 0.3947\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3613 - val_loss: 0.3906\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3594 - val_loss: 0.3905\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3585 - val_loss: 0.3881\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3568 - val_loss: 0.3924\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3552 - val_loss: 0.3888\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3543 - val_loss: 0.3871\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3530 - val_loss: 0.3854\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3518 - val_loss: 0.3875\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3513 - val_loss: 0.3867\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3496 - val_loss: 0.3855\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3491 - val_loss: 0.3893\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3481 - val_loss: 0.3840\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3468 - val_loss: 0.3871\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3459 - val_loss: 0.3834\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3447 - val_loss: 0.3818\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3440 - val_loss: 0.3845\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3430 - val_loss: 0.3806\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3423 - val_loss: 0.3778\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3418 - val_loss: 0.3783\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3410 - val_loss: 0.3761\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3403 - val_loss: 0.3743\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3392 - val_loss: 0.3754\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3390 - val_loss: 0.3741\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3377 - val_loss: 0.3749\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3372 - val_loss: 0.3736\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3371 - val_loss: 0.3683\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3362 - val_loss: 0.3707\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3353 - val_loss: 0.3718\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3349 - val_loss: 0.3709\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3340 - val_loss: 0.3679\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3334 - val_loss: 0.3663\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3328 - val_loss: 0.3659\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3327 - val_loss: 0.3704\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3319 - val_loss: 0.3627\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3311 - val_loss: 0.3650\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3305 - val_loss: 0.3634\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3299 - val_loss: 0.3652\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.3299 - val_loss: 0.3638\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3287 - val_loss: 0.3642\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3281 - val_loss: 0.3620\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3280 - val_loss: 0.3619\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3274 - val_loss: 0.3610\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3272 - val_loss: 0.3607\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3268 - val_loss: 0.3603\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3262 - val_loss: 0.3561\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3257 - val_loss: 0.3568\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3253 - val_loss: 0.3596\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3251 - val_loss: 0.3559\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3246 - val_loss: 0.3569\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3235 - val_loss: 0.3585\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3233 - val_loss: 0.3606\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3233 - val_loss: 0.3607\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3225 - val_loss: 0.3570\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3224 - val_loss: 0.3552\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3221 - val_loss: 0.3555\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3217 - val_loss: 0.3553\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3214 - val_loss: 0.3548\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3205 - val_loss: 0.3551\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3202 - val_loss: 0.3567\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3201 - val_loss: 0.3542\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3200 - val_loss: 0.3522\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3195 - val_loss: 0.3568\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3194 - val_loss: 0.3531\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3188 - val_loss: 0.3520\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3185 - val_loss: 0.3530\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3182 - val_loss: 0.3549\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3177 - val_loss: 0.3489\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3178 - val_loss: 0.3512\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3232\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 0.3151\n",
      "[CV]  n_neurons=25, n_hidden=2, learning_rate=0.0028297426043239157, total=  43.1s\n",
      "[CV] n_neurons=44, n_hidden=2, learning_rate=0.010390988229006323 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 108us/sample - loss: 0.9140 - val_loss: 0.7346\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.5020 - val_loss: 0.5145\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4545 - val_loss: 1.5197\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.4690 - val_loss: 1.2532\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.4803 - val_loss: 0.8014\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4024 - val_loss: 0.7551\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4066 - val_loss: 0.4222\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3615 - val_loss: 0.4063\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3532 - val_loss: 0.4047\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3496 - val_loss: 0.4061\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3443 - val_loss: 0.3903\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3419 - val_loss: 0.3893\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3383 - val_loss: 0.3844\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3345 - val_loss: 0.3820\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3339 - val_loss: 0.3787\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3307 - val_loss: 0.3797\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3274 - val_loss: 0.3840\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3256 - val_loss: 0.3813\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3246 - val_loss: 0.3687\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3217 - val_loss: 0.3718\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3206 - val_loss: 0.3791\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3178 - val_loss: 0.3785\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3157 - val_loss: 0.3634\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3141 - val_loss: 0.3667\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3131 - val_loss: 0.3616\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3117 - val_loss: 0.3630\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3116 - val_loss: 0.3603\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3099 - val_loss: 0.3632\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3079 - val_loss: 0.3609\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3061 - val_loss: 0.3569\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3045 - val_loss: 0.3613\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3040 - val_loss: 0.3551\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3013 - val_loss: 0.3624\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2999 - val_loss: 0.3523\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2983 - val_loss: 0.3575\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2971 - val_loss: 0.3485\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2958 - val_loss: 0.3448\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2950 - val_loss: 0.3461\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2929 - val_loss: 0.3407\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.2923 - val_loss: 0.3472\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2906 - val_loss: 0.3437\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2890 - val_loss: 0.3357\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2870 - val_loss: 0.3478\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2858 - val_loss: 0.3392\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2844 - val_loss: 0.3336\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2844 - val_loss: 0.3370\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2822 - val_loss: 0.3320\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2817 - val_loss: 0.3321\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2793 - val_loss: 0.3375\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2783 - val_loss: 0.3322\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2789 - val_loss: 0.3237\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2771 - val_loss: 0.3327\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2750 - val_loss: 0.3393\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2737 - val_loss: 0.3300\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2746 - val_loss: 0.3243\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2726 - val_loss: 0.3309\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2724 - val_loss: 0.3217\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2713 - val_loss: 0.3273\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2728 - val_loss: 0.3244\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2711 - val_loss: 0.3184\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2687 - val_loss: 0.3264\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2697 - val_loss: 0.3214\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2681 - val_loss: 0.3235\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2678 - val_loss: 0.3215\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2661 - val_loss: 0.3191\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2659 - val_loss: 0.3166\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2647 - val_loss: 0.3257\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2643 - val_loss: 0.3182\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2630 - val_loss: 0.3262\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2623 - val_loss: 0.3053\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2608 - val_loss: 0.3330\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2618 - val_loss: 0.3073\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2616 - val_loss: 0.3370\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2609 - val_loss: 0.3193\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2606 - val_loss: 0.3406\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2589 - val_loss: 0.3032\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2592 - val_loss: 0.3452\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2592 - val_loss: 0.3027\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2581 - val_loss: 0.3478\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2584 - val_loss: 0.3126\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2557 - val_loss: 0.3465\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2558 - val_loss: 0.3038\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2555 - val_loss: 0.3316\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2560 - val_loss: 0.3053\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2538 - val_loss: 0.3301\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2551 - val_loss: 0.3091\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2538 - val_loss: 0.3171\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2532 - val_loss: 0.3007\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2522 - val_loss: 0.3186\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2526 - val_loss: 0.3028\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2526 - val_loss: 0.3137\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2519 - val_loss: 0.3015\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2524 - val_loss: 0.3228\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2526 - val_loss: 0.2989\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2505 - val_loss: 0.3275\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2517 - val_loss: 0.2982\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2511 - val_loss: 0.3251\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2489 - val_loss: 0.2994\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2504 - val_loss: 0.3215\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2494 - val_loss: 0.3048\n",
      "3870/3870 [==============================] - 0s 25us/sample - loss: 0.2831\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 0.2514\n",
      "[CV]  n_neurons=44, n_hidden=2, learning_rate=0.010390988229006323, total=  42.8s\n",
      "[CV] n_neurons=44, n_hidden=2, learning_rate=0.010390988229006323 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 111us/sample - loss: 0.8494 - val_loss: 2.1264\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.4745 - val_loss: 0.5845\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.4134 - val_loss: 0.4554\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3899 - val_loss: 0.4519\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3850 - val_loss: 0.4238\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3702 - val_loss: 0.4065\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3616 - val_loss: 0.3982\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3581 - val_loss: 0.4257\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3507 - val_loss: 0.3859\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3470 - val_loss: 0.3928\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3411 - val_loss: 0.3963\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3381 - val_loss: 0.3996\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3358 - val_loss: 0.4041\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3465 - val_loss: 0.4000\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3292 - val_loss: 0.3837\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3323 - val_loss: 0.4258\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3301 - val_loss: 0.3628\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3223 - val_loss: 0.3863\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3227 - val_loss: 0.3730\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3183 - val_loss: 0.3569\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3156 - val_loss: 0.3773\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3140 - val_loss: 0.3642\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3132 - val_loss: 0.3792\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3108 - val_loss: 0.3540\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3087 - val_loss: 0.3773\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3069 - val_loss: 0.3518\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3049 - val_loss: 0.3541\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3061 - val_loss: 0.3582\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3008 - val_loss: 0.3817\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3019 - val_loss: 0.3401\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3047 - val_loss: 0.3537\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2962 - val_loss: 0.3371\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2991 - val_loss: 0.3627\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2941 - val_loss: 0.3373\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2977 - val_loss: 0.3636\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2925 - val_loss: 0.3328\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2919 - val_loss: 0.3568\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2899 - val_loss: 0.3400\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2873 - val_loss: 0.3328\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2865 - val_loss: 0.3338\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2854 - val_loss: 0.3412\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2857 - val_loss: 0.3405\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2851 - val_loss: 0.3296\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.2823 - val_loss: 0.3330\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2823 - val_loss: 0.3252\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2813 - val_loss: 0.3552\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2812 - val_loss: 0.3238\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2784 - val_loss: 0.3301\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2857 - val_loss: 0.3324\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2815 - val_loss: 0.3283\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2771 - val_loss: 0.3355\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2749 - val_loss: 0.3459\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2736 - val_loss: 0.3641\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2748 - val_loss: 0.3359\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2737 - val_loss: 0.3503\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2714 - val_loss: 0.3166\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.2717 - val_loss: 0.3197\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2711 - val_loss: 0.3413\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2733 - val_loss: 0.3687\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2747 - val_loss: 0.5803\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3927 - val_loss: 0.3417\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.2775 - val_loss: 0.3313\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2740 - val_loss: 0.3473\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2724 - val_loss: 0.3433\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2713 - val_loss: 0.3451\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2702 - val_loss: 0.3323\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.2949\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: 0.2638\n",
      "[CV]  n_neurons=44, n_hidden=2, learning_rate=0.010390988229006323, total=  30.6s\n",
      "[CV] n_neurons=44, n_hidden=2, learning_rate=0.010390988229006323 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 129us/sample - loss: 0.7564 - val_loss: 0.6421\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.4391 - val_loss: 0.5357\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.4046 - val_loss: 0.4347\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: 0.3902 - val_loss: 0.4449\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: 0.3779 - val_loss: 0.4382\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.3719 - val_loss: 0.4059\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3661 - val_loss: 0.4079\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3625 - val_loss: 0.4350\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3578 - val_loss: 0.3917\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3538 - val_loss: 0.3930\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3515 - val_loss: 0.4608\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3487 - val_loss: 0.3732\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3442 - val_loss: 0.4131\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3422 - val_loss: 0.3880\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3384 - val_loss: 0.3785\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3346 - val_loss: 0.3807\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3326 - val_loss: 0.3592\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3301 - val_loss: 0.3914\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3274 - val_loss: 0.3950\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3251 - val_loss: 0.6006\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3256 - val_loss: 0.4715\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3225 - val_loss: 0.6455\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3228 - val_loss: 0.5095\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.3211 - val_loss: 0.7587\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3191 - val_loss: 0.5482\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3133 - val_loss: 0.6281\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3099 - val_loss: 0.4252\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3302\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 0.3071\n",
      "[CV]  n_neurons=44, n_hidden=2, learning_rate=0.010390988229006323, total=  13.6s\n",
      "[CV] n_neurons=35, n_hidden=2, learning_rate=0.015375618870770533 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 134us/sample - loss: 1.0552 - val_loss: 7.8138\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1420.7533 - val_loss: 158.2625\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 13.1484 - val_loss: 1.3255\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 62us/sample - loss: 1.3059 - val_loss: 1.3286\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 1.2889 - val_loss: 1.3268\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.2858 - val_loss: 1.3207\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 1.2764 - val_loss: 1.3106\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 1.2703 - val_loss: 1.3105\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.2669 - val_loss: 1.2774\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.2407 - val_loss: 1.2134\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 1.1974 - val_loss: 1.2992\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.1656 - val_loss: 1.0985\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.0800 - val_loss: 1.1175\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.0558 - val_loss: 1.1292\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 1.0506 - val_loss: 1.0708\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.0407 - val_loss: 1.0661\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.1236 - val_loss: 1.0671\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.0304 - val_loss: 1.0870\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 1.0288 - val_loss: 1.0753\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.0298 - val_loss: 1.2038\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.0273 - val_loss: 1.0768\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.0249 - val_loss: 1.0536\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 1.0246 - val_loss: 1.0604\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.0224 - val_loss: 1.0808\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.0212 - val_loss: 1.0535\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 1.0209 - val_loss: 1.0518\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.0206 - val_loss: 1.0502\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.0184 - val_loss: 1.0518\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.0210 - val_loss: 1.0543\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 1.0206 - val_loss: 1.0554\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 1.0177 - val_loss: 1.0889\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.9994 - val_loss: 1.0501\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.9172 - val_loss: 0.9787\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.9001 - val_loss: 1.0506\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.9022 - val_loss: 0.9001\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8969 - val_loss: 0.8906\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8989 - val_loss: 0.8906\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8950 - val_loss: 0.9000\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8903 - val_loss: 0.9003\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8787 - val_loss: 0.8852\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8795 - val_loss: 0.8823\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.8842 - val_loss: 0.8838\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8868 - val_loss: 0.8826\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8791 - val_loss: 0.8938\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8771 - val_loss: 0.8807\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8804 - val_loss: 0.8900\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.8715 - val_loss: 0.9418\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8782 - val_loss: 0.9474\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8700 - val_loss: 0.9498\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8676 - val_loss: 0.8889\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.8704 - val_loss: 0.9067\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8690 - val_loss: 0.8837\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8666 - val_loss: 0.8827\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.8630 - val_loss: 0.8746\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.8691 - val_loss: 0.9055\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8611 - val_loss: 0.8821\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8618 - val_loss: 0.8672\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8576 - val_loss: 0.8762\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8529 - val_loss: 1.0530\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8607 - val_loss: 0.8848\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8447 - val_loss: 0.8975\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.8562 - val_loss: 0.8741\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.8557 - val_loss: 0.8715\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8451 - val_loss: 0.8683\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8391 - val_loss: 0.8487\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.8831 - val_loss: 1.0551\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 1.0063 - val_loss: 1.0428\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 1.0039 - val_loss: 1.0429\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 1.0046 - val_loss: 1.0424\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 1.0034 - val_loss: 1.0450\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.0028 - val_loss: 1.0478\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 1.0051 - val_loss: 1.0427\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.0030 - val_loss: 1.0439\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.0027 - val_loss: 1.0421\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 1.0032 - val_loss: 1.0457\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 1.0196\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.0008\n",
      "[CV]  n_neurons=35, n_hidden=2, learning_rate=0.015375618870770533, total=  33.4s\n",
      "[CV] n_neurons=35, n_hidden=2, learning_rate=0.015375618870770533 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 120us/sample - loss: 0.9373 - val_loss: 0.9451\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4847 - val_loss: 0.4494\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4053 - val_loss: 0.4330\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3839 - val_loss: 0.4139\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3712 - val_loss: 0.4299\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3651 - val_loss: 0.4082\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3547 - val_loss: 0.4002\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3478 - val_loss: 0.3924\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3400 - val_loss: 0.3837\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3355 - val_loss: 0.3797\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3334 - val_loss: 0.3852\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3298 - val_loss: 0.3812\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3267 - val_loss: 0.3748\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3224 - val_loss: 0.3743\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3174 - val_loss: 0.3706\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3166 - val_loss: 0.3765\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3140 - val_loss: 0.3654\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - ETA: 0s - loss: 0.310 - 0s 59us/sample - loss: 0.3123 - val_loss: 0.3631\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3110 - val_loss: 0.3600\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3070 - val_loss: 0.3520\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3049 - val_loss: 0.3529\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3008 - val_loss: 0.3517\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2985 - val_loss: 0.3688\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2979 - val_loss: 0.3451\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2949 - val_loss: 0.3425\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2956 - val_loss: 0.3432\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2941 - val_loss: 0.3518\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2914 - val_loss: 0.3496\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2905 - val_loss: 0.3418\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2893 - val_loss: 0.3431\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2882 - val_loss: 0.3356\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2882 - val_loss: 0.3480\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2878 - val_loss: 0.3351\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.2847 - val_loss: 0.3420\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2835 - val_loss: 0.3447\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2817 - val_loss: 0.3336\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2828 - val_loss: 0.3321\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2817 - val_loss: 0.3372\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2801 - val_loss: 0.3384\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2795 - val_loss: 0.3339\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2781 - val_loss: 0.3349\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2786 - val_loss: 0.3338\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2762 - val_loss: 0.3313\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2748 - val_loss: 0.3334\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2749 - val_loss: 0.3395\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2755 - val_loss: 0.3412\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2764 - val_loss: 0.3232\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2756 - val_loss: 0.3283\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2733 - val_loss: 0.3380\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2722 - val_loss: 0.3247\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2721 - val_loss: 0.3343\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2714 - val_loss: 0.3297\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2705 - val_loss: 0.3292\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2704 - val_loss: 0.3275\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2693 - val_loss: 0.3211\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2686 - val_loss: 0.3233\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2672 - val_loss: 0.3217\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2685 - val_loss: 0.3201\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2662 - val_loss: 0.3257\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2675 - val_loss: 0.3230\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2658 - val_loss: 0.3331\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2640 - val_loss: 0.3245\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2643 - val_loss: 0.3167\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2657 - val_loss: 0.3334\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2632 - val_loss: 0.3187\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2627 - val_loss: 0.3191\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2631 - val_loss: 0.3201\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2625 - val_loss: 0.3202\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2621 - val_loss: 0.3158\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2624 - val_loss: 0.3131\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2609 - val_loss: 0.3151\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2605 - val_loss: 0.3182\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2625 - val_loss: 0.3144\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2594 - val_loss: 0.3332\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2610 - val_loss: 0.3118\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2599 - val_loss: 0.3389\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2604 - val_loss: 0.3089\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2585 - val_loss: 0.3269\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2583 - val_loss: 0.3271\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2593 - val_loss: 0.3177\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2600 - val_loss: 0.3245\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2594 - val_loss: 0.3251\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2569 - val_loss: 0.3084\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2581 - val_loss: 0.3180\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2555 - val_loss: 0.3087\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2573 - val_loss: 0.3269\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2550 - val_loss: 0.3153\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2551 - val_loss: 0.3163\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2561 - val_loss: 0.3156\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2530 - val_loss: 0.3206\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2550 - val_loss: 0.3195\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2517 - val_loss: 0.3126\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2540 - val_loss: 0.3234\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3050\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: 0.2660\n",
      "[CV]  n_neurons=35, n_hidden=2, learning_rate=0.015375618870770533, total=  41.0s\n",
      "[CV] n_neurons=35, n_hidden=2, learning_rate=0.015375618870770533 ....\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 123us/sample - loss: 0.7197 - val_loss: 0.8683\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.4352 - val_loss: 0.4495\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3897 - val_loss: 0.4405\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3745 - val_loss: 0.4293\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3652 - val_loss: 0.4214\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3602 - val_loss: 0.3916\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3545 - val_loss: 0.4128\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3515 - val_loss: 0.4136\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3462 - val_loss: 0.4087\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3418 - val_loss: 0.3976\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3391 - val_loss: 0.3880\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3338 - val_loss: 0.3770\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3310 - val_loss: 0.3752\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3295 - val_loss: 0.3954\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3268 - val_loss: 0.3653\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3231 - val_loss: 0.3916\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3203 - val_loss: 0.3621\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3169 - val_loss: 0.3641\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3160 - val_loss: 0.3669\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3145 - val_loss: 0.3539\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3116 - val_loss: 0.3665\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3123 - val_loss: 0.3550\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3061 - val_loss: 0.3468\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3061 - val_loss: 0.3519\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.3043 - val_loss: 0.3281\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.3005 - val_loss: 0.3915\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.3003 - val_loss: 0.3524\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3001 - val_loss: 0.4078\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2996 - val_loss: 0.3579\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2949 - val_loss: 0.3546\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2944 - val_loss: 0.3343\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2931 - val_loss: 0.3675\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2921 - val_loss: 0.3194\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2918 - val_loss: 0.4042\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2890 - val_loss: 0.3302\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2883 - val_loss: 0.4205\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2882 - val_loss: 0.3386\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2873 - val_loss: 0.3648\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2840 - val_loss: 0.3255\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2832 - val_loss: 0.3536\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2830 - val_loss: 0.3201\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2835 - val_loss: 0.3450\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2821 - val_loss: 0.3173\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2818 - val_loss: 0.3941\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2792 - val_loss: 0.3160\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2789 - val_loss: 0.3531\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2777 - val_loss: 0.3201\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2771 - val_loss: 0.3271\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2765 - val_loss: 0.3500\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2764 - val_loss: 0.3168\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2750 - val_loss: 0.3531\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2738 - val_loss: 0.3150\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2750 - val_loss: 0.3991\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2741 - val_loss: 0.3151\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2746 - val_loss: 0.3772\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2732 - val_loss: 0.5656\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2731 - val_loss: 0.6085\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 54us/sample - loss: 0.2714 - val_loss: 0.3905\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2715 - val_loss: 0.4707\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 0.2708 - val_loss: 0.4026\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2703 - val_loss: 0.4511\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 55us/sample - loss: 0.2707 - val_loss: 0.3540\n",
      "3870/3870 [==============================] - 0s 24us/sample - loss: 0.3027\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 0.2601\n",
      "[CV]  n_neurons=35, n_hidden=2, learning_rate=0.015375618870770533, total=  27.6s\n",
      "[CV] n_neurons=59, n_hidden=0, learning_rate=0.0003166202999165769 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 116us/sample - loss: 6.6312 - val_loss: 10.7057\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 4.8667 - val_loss: 7.7155\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 3.6867 - val_loss: 5.6499\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 2.8737 - val_loss: 4.2172\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 2.3015 - val_loss: 3.2208\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 1.8934 - val_loss: 2.5209\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 1.5987 - val_loss: 2.0296\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 1.3842 - val_loss: 1.6822\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.2267 - val_loss: 1.4362\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.1102 - val_loss: 1.2614\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 1.0233 - val_loss: 1.1336\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.9579 - val_loss: 1.0420\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.9083 - val_loss: 0.9754\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.8701 - val_loss: 0.9251\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.8404 - val_loss: 0.8875\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.8170 - val_loss: 0.8590\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.7982 - val_loss: 0.8368\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7829 - val_loss: 0.8195\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7701 - val_loss: 0.8053\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7592 - val_loss: 0.7938\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.7498 - val_loss: 0.7839\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7415 - val_loss: 0.7752\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7341 - val_loss: 0.7675\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.7273 - val_loss: 0.7607\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7211 - val_loss: 0.7544\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7153 - val_loss: 0.7487\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.7098 - val_loss: 0.7432\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.7046 - val_loss: 0.7381\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6997 - val_loss: 0.7332\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6949 - val_loss: 0.7286\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6904 - val_loss: 0.7240\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6860 - val_loss: 0.7196\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6817 - val_loss: 0.7155\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6776 - val_loss: 0.7114\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6736 - val_loss: 0.7073\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6696 - val_loss: 0.7035\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6658 - val_loss: 0.6997\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6621 - val_loss: 0.6960\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6586 - val_loss: 0.6923\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6550 - val_loss: 0.6888\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6516 - val_loss: 0.6854\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6482 - val_loss: 0.6821\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6450 - val_loss: 0.6788\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6418 - val_loss: 0.6755\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6387 - val_loss: 0.6725\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6356 - val_loss: 0.6694\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6327 - val_loss: 0.6665\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6298 - val_loss: 0.6636\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6270 - val_loss: 0.6608\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6242 - val_loss: 0.6580\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6215 - val_loss: 0.6553\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6189 - val_loss: 0.6525\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6164 - val_loss: 0.6498\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6138 - val_loss: 0.6472\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6114 - val_loss: 0.6448\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6090 - val_loss: 0.6423\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6067 - val_loss: 0.6399\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6044 - val_loss: 0.6376\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6022 - val_loss: 0.6354\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6000 - val_loss: 0.6331\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5979 - val_loss: 0.6308\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5958 - val_loss: 0.6286\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5938 - val_loss: 0.6264\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5918 - val_loss: 0.6244\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5899 - val_loss: 0.6224\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5880 - val_loss: 0.6205\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5862 - val_loss: 0.6187\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5843 - val_loss: 0.6168\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5826 - val_loss: 0.6149\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5809 - val_loss: 0.6131\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5792 - val_loss: 0.6113\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5776 - val_loss: 0.6096\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5760 - val_loss: 0.6079\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5745 - val_loss: 0.6063\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5728 - val_loss: 0.6047\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5714 - val_loss: 0.6031\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5700 - val_loss: 0.6016\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5686 - val_loss: 0.6000\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5672 - val_loss: 0.5986\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5658 - val_loss: 0.5971\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5645 - val_loss: 0.5958\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5632 - val_loss: 0.5944\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5620 - val_loss: 0.5930\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5607 - val_loss: 0.5917\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5595 - val_loss: 0.5904\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5583 - val_loss: 0.5891\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5572 - val_loss: 0.5878\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5561 - val_loss: 0.5866\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5550 - val_loss: 0.5855\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5539 - val_loss: 0.5844\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5529 - val_loss: 0.5832\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5519 - val_loss: 0.5820\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5508 - val_loss: 0.5810\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5499 - val_loss: 0.5800\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5489 - val_loss: 0.5789\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5480 - val_loss: 0.5779\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5471 - val_loss: 0.5769\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5462 - val_loss: 0.5758\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5454 - val_loss: 0.5749\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5445 - val_loss: 0.5740\n",
      "3870/3870 [==============================] - 0s 22us/sample - loss: 0.5640\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 0.5435\n",
      "[CV]  n_neurons=59, n_hidden=0, learning_rate=0.0003166202999165769, total=  38.3s\n",
      "[CV] n_neurons=59, n_hidden=0, learning_rate=0.0003166202999165769 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 116us/sample - loss: 5.5455 - val_loss: 5.8603\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 4.1039 - val_loss: 4.3038\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 3.1219 - val_loss: 3.2497\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 2.4404 - val_loss: 2.5241\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 1.9592 - val_loss: 2.0179\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 1.6149 - val_loss: 1.6588\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 1.3656 - val_loss: 1.3986\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.1839 - val_loss: 1.2118\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 1.0505 - val_loss: 1.0763\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.9521 - val_loss: 0.9777\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.8790 - val_loss: 0.9055\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.8246 - val_loss: 0.8510\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.7835 - val_loss: 0.8101\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7524 - val_loss: 0.7790\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: 0.7287 - val_loss: 0.7553\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.7103 - val_loss: 0.7365\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6958 - val_loss: 0.7225\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6844 - val_loss: 0.7113\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6752 - val_loss: 0.7019\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6676 - val_loss: 0.6938\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6612 - val_loss: 0.6870\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6557 - val_loss: 0.6816\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6510 - val_loss: 0.6764\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6468 - val_loss: 0.6721\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6430 - val_loss: 0.6680\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6395 - val_loss: 0.6642\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6363 - val_loss: 0.6606\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6333 - val_loss: 0.6573\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6304 - val_loss: 0.6542\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6277 - val_loss: 0.6514\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6251 - val_loss: 0.6488\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6226 - val_loss: 0.6461\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6202 - val_loss: 0.6435\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6179 - val_loss: 0.6408\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6156 - val_loss: 0.6383\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6133 - val_loss: 0.6360\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6112 - val_loss: 0.6337\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6091 - val_loss: 0.6316\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6071 - val_loss: 0.6293\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6051 - val_loss: 0.6273\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6032 - val_loss: 0.6253\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6013 - val_loss: 0.6233\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5995 - val_loss: 0.6212\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5977 - val_loss: 0.6191\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5960 - val_loss: 0.6172\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5942 - val_loss: 0.6155\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5926 - val_loss: 0.6135\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5910 - val_loss: 0.6116\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5894 - val_loss: 0.6099\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5878 - val_loss: 0.6083\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5863 - val_loss: 0.6066\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5848 - val_loss: 0.6051\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5833 - val_loss: 0.6036\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5820 - val_loss: 0.6022\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5806 - val_loss: 0.6006\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5793 - val_loss: 0.5990\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5779 - val_loss: 0.5977\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5767 - val_loss: 0.5962\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5754 - val_loss: 0.5948\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5741 - val_loss: 0.5936\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5730 - val_loss: 0.5921\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5718 - val_loss: 0.5909\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5707 - val_loss: 0.5896\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5696 - val_loss: 0.5883\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5684 - val_loss: 0.5871\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5674 - val_loss: 0.5858\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5664 - val_loss: 0.5846\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5653 - val_loss: 0.5835\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5643 - val_loss: 0.5824\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5634 - val_loss: 0.5812\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5624 - val_loss: 0.5801\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5615 - val_loss: 0.5791\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5606 - val_loss: 0.5782\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5597 - val_loss: 0.5772\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5588 - val_loss: 0.5763\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5580 - val_loss: 0.5752\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5572 - val_loss: 0.5742\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5564 - val_loss: 0.5732\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5556 - val_loss: 0.5724\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5548 - val_loss: 0.5716\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5541 - val_loss: 0.5707\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5534 - val_loss: 0.5698\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5527 - val_loss: 0.5689\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5519 - val_loss: 0.5681\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5513 - val_loss: 0.5674\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5506 - val_loss: 0.5665\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5499 - val_loss: 0.5658\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5493 - val_loss: 0.5650\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5487 - val_loss: 0.5644\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5481 - val_loss: 0.5637\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5475 - val_loss: 0.5630\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5469 - val_loss: 0.5622\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5463 - val_loss: 0.5616\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5457 - val_loss: 0.5610\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5452 - val_loss: 0.5603\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5447 - val_loss: 0.5596\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5442 - val_loss: 0.5589\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5437 - val_loss: 0.5583\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5432 - val_loss: 0.5577\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5427 - val_loss: 0.5572\n",
      "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5237\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: 0.5418\n",
      "[CV]  n_neurons=59, n_hidden=0, learning_rate=0.0003166202999165769, total=  38.2s\n",
      "[CV] n_neurons=59, n_hidden=0, learning_rate=0.0003166202999165769 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 119us/sample - loss: 5.9989 - val_loss: 7.7635\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 4.5544 - val_loss: 6.3002\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 3.5125 - val_loss: 5.1911\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 2.7567 - val_loss: 4.3396\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 2.2058 - val_loss: 3.6748\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 1.8019 - val_loss: 3.1514\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 1.5046 - val_loss: 2.7326\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 1.2848 - val_loss: 2.3937\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 1.1214 - val_loss: 2.1161\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.9991 - val_loss: 1.8859\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.9071 - val_loss: 1.6932\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.8375 - val_loss: 1.5309\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7845 - val_loss: 1.3932\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.7438 - val_loss: 1.2751\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.7122 - val_loss: 1.1740\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6875 - val_loss: 1.0870\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6679 - val_loss: 1.0115\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6523 - val_loss: 0.9460\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6396 - val_loss: 0.8893\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6293 - val_loss: 0.8402\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6207 - val_loss: 0.7979\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6134 - val_loss: 0.7610\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6072 - val_loss: 0.7293\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6019 - val_loss: 0.7024\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5972 - val_loss: 0.6794\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5931 - val_loss: 0.6597\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5894 - val_loss: 0.6434\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5861 - val_loss: 0.6299\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5831 - val_loss: 0.6188\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5804 - val_loss: 0.6099\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5778 - val_loss: 0.6029\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5754 - val_loss: 0.5976\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5732 - val_loss: 0.5938\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5712 - val_loss: 0.5914\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5692 - val_loss: 0.5901\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5674 - val_loss: 0.5898\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5656 - val_loss: 0.5904\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5640 - val_loss: 0.5917\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5624 - val_loss: 0.5937\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5609 - val_loss: 0.5962\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5595 - val_loss: 0.5992\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5581 - val_loss: 0.6026\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5567 - val_loss: 0.6064\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5555 - val_loss: 0.6105\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5542 - val_loss: 0.6147\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5531 - val_loss: 0.6188\n",
      "3870/3870 [==============================] - 0s 22us/sample - loss: 0.5593\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: 0.5524\n",
      "[CV]  n_neurons=59, n_hidden=0, learning_rate=0.0003166202999165769, total=  18.1s\n",
      "[CV] n_neurons=73, n_hidden=0, learning_rate=0.0004525167258753754 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 123us/sample - loss: 6.3881 - val_loss: 7.9682\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 4.1928 - val_loss: 5.0256\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 2.8807 - val_loss: 3.2989\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 2.0711 - val_loss: 2.2701\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.5602 - val_loss: 1.6571\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 1.2336 - val_loss: 1.2808\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.0222 - val_loss: 1.0488\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.8844 - val_loss: 0.9031\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7934 - val_loss: 0.8117\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.7330 - val_loss: 0.7528\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6920 - val_loss: 0.7146\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6639 - val_loss: 0.6885\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6442 - val_loss: 0.6704\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6300 - val_loss: 0.6576\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6195 - val_loss: 0.6481\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6115 - val_loss: 0.6410\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6051 - val_loss: 0.6346\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6000 - val_loss: 0.6298\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5956 - val_loss: 0.6257\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5917 - val_loss: 0.6222\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5883 - val_loss: 0.6189\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5852 - val_loss: 0.6163\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5824 - val_loss: 0.6132\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5796 - val_loss: 0.6110\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5771 - val_loss: 0.6088\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5747 - val_loss: 0.6059\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5724 - val_loss: 0.6037\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5703 - val_loss: 0.6012\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5681 - val_loss: 0.5987\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5660 - val_loss: 0.5971\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5641 - val_loss: 0.5952\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5623 - val_loss: 0.5932\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5605 - val_loss: 0.5913\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5587 - val_loss: 0.5893\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5570 - val_loss: 0.5877\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5554 - val_loss: 0.5859\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5538 - val_loss: 0.5841\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5523 - val_loss: 0.5828\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5508 - val_loss: 0.5813\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5494 - val_loss: 0.5796\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5480 - val_loss: 0.5784\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5467 - val_loss: 0.5768\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5454 - val_loss: 0.5753\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5441 - val_loss: 0.5744\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5430 - val_loss: 0.5730\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5418 - val_loss: 0.5717\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5407 - val_loss: 0.5708\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5397 - val_loss: 0.5692\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5386 - val_loss: 0.5683\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5376 - val_loss: 0.5673\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5366 - val_loss: 0.5665\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5357 - val_loss: 0.5652\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5347 - val_loss: 0.5644\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5339 - val_loss: 0.5634\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5330 - val_loss: 0.5626\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5322 - val_loss: 0.5612\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5314 - val_loss: 0.5605\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5306 - val_loss: 0.5598\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5299 - val_loss: 0.5585\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5292 - val_loss: 0.5577\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5285 - val_loss: 0.5567\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5278 - val_loss: 0.5559\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5272 - val_loss: 0.5549\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5265 - val_loss: 0.5542\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5259 - val_loss: 0.5535\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5253 - val_loss: 0.5528\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5248 - val_loss: 0.5521\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5242 - val_loss: 0.5514\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5237 - val_loss: 0.5506\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5232 - val_loss: 0.5500\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5226 - val_loss: 0.5496\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5222 - val_loss: 0.5491\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5217 - val_loss: 0.5485\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5212 - val_loss: 0.5482\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5209 - val_loss: 0.5475\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5204 - val_loss: 0.5468\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5200 - val_loss: 0.5465\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5195 - val_loss: 0.5463\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5193 - val_loss: 0.5457\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5189 - val_loss: 0.5451\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5185 - val_loss: 0.5447\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5181 - val_loss: 0.5443\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5178 - val_loss: 0.5438\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5175 - val_loss: 0.5436\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5172 - val_loss: 0.5433\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5169 - val_loss: 0.5430\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5166 - val_loss: 0.5424\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5164 - val_loss: 0.5418\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5161 - val_loss: 0.5414\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5158 - val_loss: 0.5410\n",
      "Epoch 91/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5156 - val_loss: 0.5408\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5153 - val_loss: 0.5405\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5151 - val_loss: 0.5402\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5149 - val_loss: 0.5398\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5146 - val_loss: 0.5395\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5143 - val_loss: 0.5394\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5142 - val_loss: 0.5390\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5140 - val_loss: 0.5388\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5138 - val_loss: 0.5385\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5136 - val_loss: 0.5382\n",
      "3870/3870 [==============================] - 0s 22us/sample - loss: 0.5343\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: 0.5128\n",
      "[CV]  n_neurons=73, n_hidden=0, learning_rate=0.0004525167258753754, total=  38.3s\n",
      "[CV] n_neurons=73, n_hidden=0, learning_rate=0.0004525167258753754 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 122us/sample - loss: 6.0087 - val_loss: 4.6316\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 3.9285 - val_loss: 3.2173\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 2.7242 - val_loss: 2.3322\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.9930 - val_loss: 1.7698\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 1.5360 - val_loss: 1.4069\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.2434 - val_loss: 1.1704\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.0539 - val_loss: 1.0150\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.9290 - val_loss: 0.9114\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.8460 - val_loss: 0.8422\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7899 - val_loss: 0.7949\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.7514 - val_loss: 0.7622\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7243 - val_loss: 0.7387\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7047 - val_loss: 0.7216\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6901 - val_loss: 0.7087\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6791 - val_loss: 0.6986\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6702 - val_loss: 0.6904\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6627 - val_loss: 0.6836\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6565 - val_loss: 0.6777\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6510 - val_loss: 0.6723\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6460 - val_loss: 0.6672\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6414 - val_loss: 0.6628\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6373 - val_loss: 0.6585\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6332 - val_loss: 0.6547\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6295 - val_loss: 0.6507\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6259 - val_loss: 0.6469\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6224 - val_loss: 0.6435\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6191 - val_loss: 0.6403\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6159 - val_loss: 0.6373\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6130 - val_loss: 0.6338\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.6100 - val_loss: 0.6308\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.6071 - val_loss: 0.6279\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.6044 - val_loss: 0.6249\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6018 - val_loss: 0.6219\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5992 - val_loss: 0.6193\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5967 - val_loss: 0.6166\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5943 - val_loss: 0.6139\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5920 - val_loss: 0.6114\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5898 - val_loss: 0.6091\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: 0.5876 - val_loss: 0.6068\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5855 - val_loss: 0.6045\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5834 - val_loss: 0.6025\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5815 - val_loss: 0.6004\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5797 - val_loss: 0.5983\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5779 - val_loss: 0.5962\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5760 - val_loss: 0.5944\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5744 - val_loss: 0.5924\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5726 - val_loss: 0.5907\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5710 - val_loss: 0.5887\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5695 - val_loss: 0.5870\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5680 - val_loss: 0.5853\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5666 - val_loss: 0.5836\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5651 - val_loss: 0.5821\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5638 - val_loss: 0.5806\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5624 - val_loss: 0.5792\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5612 - val_loss: 0.5777\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5600 - val_loss: 0.5763\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5588 - val_loss: 0.5749\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5576 - val_loss: 0.5737\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5565 - val_loss: 0.5724\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5555 - val_loss: 0.5711\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5543 - val_loss: 0.5698\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5533 - val_loss: 0.5688\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5524 - val_loss: 0.5675\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5514 - val_loss: 0.5664\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5505 - val_loss: 0.5654\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5496 - val_loss: 0.5644\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5488 - val_loss: 0.5633\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5479 - val_loss: 0.5624\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5471 - val_loss: 0.5615\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5464 - val_loss: 0.5604\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5456 - val_loss: 0.5596\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5448 - val_loss: 0.5587\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5442 - val_loss: 0.5578\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5435 - val_loss: 0.5569\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5428 - val_loss: 0.5561\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 49us/sample - loss: 0.5422 - val_loss: 0.5554\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5416 - val_loss: 0.5546\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5410 - val_loss: 0.5538\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5403 - val_loss: 0.5532\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5398 - val_loss: 0.5524\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5393 - val_loss: 0.5518\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: 0.5388 - val_loss: 0.5511\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5382 - val_loss: 0.5504\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5377 - val_loss: 0.5499\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5372 - val_loss: 0.5494\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5368 - val_loss: 0.5487\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: 0.5363 - val_loss: 0.5482\n",
      "Epoch 88/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5359 - val_loss: 0.5477\n",
      "Epoch 89/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5354 - val_loss: 0.5472\n",
      "Epoch 90/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5351 - val_loss: 0.5465\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5347 - val_loss: 0.5461\n",
      "Epoch 92/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5344 - val_loss: 0.5456\n",
      "Epoch 93/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5339 - val_loss: 0.5451\n",
      "Epoch 94/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5337 - val_loss: 0.5446\n",
      "Epoch 95/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5332 - val_loss: 0.5441\n",
      "Epoch 96/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5329 - val_loss: 0.5437\n",
      "Epoch 97/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5326 - val_loss: 0.5432\n",
      "Epoch 98/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5322 - val_loss: 0.5428\n",
      "Epoch 99/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5320 - val_loss: 0.5423\n",
      "Epoch 100/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5315 - val_loss: 0.5420\n",
      "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5091\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: 0.5303\n",
      "[CV]  n_neurons=73, n_hidden=0, learning_rate=0.0004525167258753754, total=  38.2s\n",
      "[CV] n_neurons=73, n_hidden=0, learning_rate=0.0004525167258753754 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 123us/sample - loss: 4.8441 - val_loss: 4.5250\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 3.3852 - val_loss: 3.2973\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 2.4388 - val_loss: 2.4785\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.8200 - val_loss: 1.9260\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.4131 - val_loss: 1.5476\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 1.1432 - val_loss: 1.2841\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.9628 - val_loss: 1.0978\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.8412 - val_loss: 0.9637\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7586 - val_loss: 0.8660\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.7019 - val_loss: 0.7934\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6624 - val_loss: 0.7390\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.6345 - val_loss: 0.6981\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6145 - val_loss: 0.6666\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.6000 - val_loss: 0.6428\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5892 - val_loss: 0.6249\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5810 - val_loss: 0.6115\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5746 - val_loss: 0.6019\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5695 - val_loss: 0.5951\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5654 - val_loss: 0.5908\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5620 - val_loss: 0.5884\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5591 - val_loss: 0.5876\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5566 - val_loss: 0.5880\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5544 - val_loss: 0.5896\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5524 - val_loss: 0.5920\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5507 - val_loss: 0.5951\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5491 - val_loss: 0.5989\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5476 - val_loss: 0.6031\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5462 - val_loss: 0.6072\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 47us/sample - loss: 0.5449 - val_loss: 0.6117\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5437 - val_loss: 0.6163\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 0.5426 - val_loss: 0.6209\n",
      "3870/3870 [==============================] - 0s 23us/sample - loss: 0.5875\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: 0.5418\n",
      "[CV]  n_neurons=73, n_hidden=0, learning_rate=0.0004525167258753754, total=  12.3s\n",
      "[CV] n_neurons=88, n_hidden=3, learning_rate=0.0043533387647423545 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 140us/sample - loss: 1.2949 - val_loss: 1.6870\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.6797 - val_loss: 0.9623\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.5105 - val_loss: 0.5061\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4433 - val_loss: 0.4706\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.4094 - val_loss: 0.4432\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3882 - val_loss: 0.4375\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.3720 - val_loss: 0.4195\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3626 - val_loss: 0.4058\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3548 - val_loss: 0.4039\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3495 - val_loss: 0.3957\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3449 - val_loss: 0.3914\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3397 - val_loss: 0.3883\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3372 - val_loss: 0.3911\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3333 - val_loss: 0.3791\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3292 - val_loss: 0.3806\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3263 - val_loss: 0.3778\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3235 - val_loss: 0.3739\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3201 - val_loss: 0.3725\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3176 - val_loss: 0.3661\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3155 - val_loss: 0.3631\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3119 - val_loss: 0.3697\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3101 - val_loss: 0.3637\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3076 - val_loss: 0.3593\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3061 - val_loss: 0.3657\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3045 - val_loss: 0.3626\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3021 - val_loss: 0.3620\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3002 - val_loss: 0.3497\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2982 - val_loss: 0.3546\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2966 - val_loss: 0.3460\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.2957 - val_loss: 0.3511\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2941 - val_loss: 0.3449\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2922 - val_loss: 0.3484\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2915 - val_loss: 0.3431\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2891 - val_loss: 0.3458\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2878 - val_loss: 0.3455\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2862 - val_loss: 0.3371\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2848 - val_loss: 0.3405\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2842 - val_loss: 0.3437\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2832 - val_loss: 0.3422\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2815 - val_loss: 0.3381\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2801 - val_loss: 0.3355\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2788 - val_loss: 0.3413\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2775 - val_loss: 0.3376\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2774 - val_loss: 0.3482\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2752 - val_loss: 0.3262\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.2754 - val_loss: 0.3338\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: 0.2742 - val_loss: 0.3248\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2722 - val_loss: 0.3425\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2720 - val_loss: 0.3240\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2714 - val_loss: 0.3703\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2722 - val_loss: 0.3363\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2715 - val_loss: 0.3935\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2711 - val_loss: 0.3256\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2700 - val_loss: 0.3810\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2686 - val_loss: 0.3196\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2673 - val_loss: 0.3538\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2669 - val_loss: 0.3218\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 1s 71us/sample - loss: 0.2659 - val_loss: 0.3428\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 1s 72us/sample - loss: 0.2650 - val_loss: 0.3169\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: 0.2626 - val_loss: 0.3386\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2641 - val_loss: 0.3160\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2616 - val_loss: 0.3361\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2616 - val_loss: 0.3138\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2613 - val_loss: 0.3256\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2599 - val_loss: 0.3195\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2597 - val_loss: 0.3253\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2588 - val_loss: 0.3164\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2577 - val_loss: 0.3374\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2576 - val_loss: 0.3136\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2577 - val_loss: 0.3192\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2553 - val_loss: 0.3164\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2565 - val_loss: 0.3243\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2538 - val_loss: 0.3253\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2554 - val_loss: 0.3289\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2533 - val_loss: 0.3085\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2536 - val_loss: 0.3280\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2538 - val_loss: 0.3084\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2536 - val_loss: 0.3601\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2536 - val_loss: 0.3229\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2533 - val_loss: 0.4016\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.2551 - val_loss: 0.3212\n",
      "Epoch 82/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2555 - val_loss: 0.3828\n",
      "Epoch 83/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2530 - val_loss: 0.3419\n",
      "Epoch 84/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2553 - val_loss: 0.3937\n",
      "Epoch 85/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2540 - val_loss: 0.3101\n",
      "Epoch 86/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2488 - val_loss: 0.3216\n",
      "Epoch 87/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2479 - val_loss: 0.3095\n",
      "3870/3870 [==============================] - 0s 27us/sample - loss: 0.2854\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: 0.2463\n",
      "[CV]  n_neurons=88, n_hidden=3, learning_rate=0.0043533387647423545, total=  41.1s\n",
      "[CV] n_neurons=88, n_hidden=3, learning_rate=0.0043533387647423545 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 146us/sample - loss: 1.0300 - val_loss: 1.0258\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.5540 - val_loss: 0.5434\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4620 - val_loss: 0.4780\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.4098 - val_loss: 0.4370\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3850 - val_loss: 0.4206\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3704 - val_loss: 0.4167\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.3590 - val_loss: 0.4080\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3532 - val_loss: 0.3959\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3444 - val_loss: 0.3929\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3400 - val_loss: 0.3905\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3353 - val_loss: 0.3892\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3307 - val_loss: 0.3758\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3259 - val_loss: 0.3728\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3234 - val_loss: 0.3704\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3205 - val_loss: 0.3612\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3172 - val_loss: 0.3573\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3133 - val_loss: 0.3701\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 0.3109 - val_loss: 0.3557\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3077 - val_loss: 0.3601\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3061 - val_loss: 0.3553\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3049 - val_loss: 0.3479\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 0.3017 - val_loss: 0.3463\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2993 - val_loss: 0.3505\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2982 - val_loss: 0.3565\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 1s 83us/sample - loss: 0.2967 - val_loss: 0.3382\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.2957 - val_loss: 0.3429\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2923 - val_loss: 0.3467\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2910 - val_loss: 0.3320\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2904 - val_loss: 0.3784\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2890 - val_loss: 0.3315\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2874 - val_loss: 0.3575\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2855 - val_loss: 0.3336\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2839 - val_loss: 0.3525\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2832 - val_loss: 0.3324\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2830 - val_loss: 0.3383\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2802 - val_loss: 0.3342\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2797 - val_loss: 0.3255\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2797 - val_loss: 0.3443\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2777 - val_loss: 0.3188\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2767 - val_loss: 0.3398\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.2764 - val_loss: 0.3281\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2741 - val_loss: 0.3518\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2731 - val_loss: 0.3321\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2734 - val_loss: 0.3325\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2723 - val_loss: 0.3342\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2705 - val_loss: 0.3406\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2725 - val_loss: 0.3254\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2691 - val_loss: 0.3682\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2724 - val_loss: 0.3273\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.2969\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 0.2631\n",
      "[CV]  n_neurons=88, n_hidden=3, learning_rate=0.0043533387647423545, total=  24.1s\n",
      "[CV] n_neurons=88, n_hidden=3, learning_rate=0.0043533387647423545 ...\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 148us/sample - loss: 0.9804 - val_loss: 0.6501\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.5331 - val_loss: 0.5552\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4697 - val_loss: 0.4843\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4314 - val_loss: 0.4507\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.4075 - val_loss: 0.4197\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3895 - val_loss: 0.4051\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3775 - val_loss: 0.4108\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3689 - val_loss: 0.3998\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3606 - val_loss: 0.4058\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3567 - val_loss: 0.3950\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 0.3516 - val_loss: 0.3960\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 0.3467 - val_loss: 0.3928\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.3430 - val_loss: 0.4039\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3402 - val_loss: 0.3943\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3354 - val_loss: 0.3922\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3313 - val_loss: 0.3853\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3290 - val_loss: 0.3752\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3263 - val_loss: 0.3746\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3225 - val_loss: 0.3805\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.3210 - val_loss: 0.3711\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3174 - val_loss: 0.3869\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.3165 - val_loss: 0.3682\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3147 - val_loss: 0.3695\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3120 - val_loss: 0.3688\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3107 - val_loss: 0.3782\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.3068 - val_loss: 0.3561\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3057 - val_loss: 0.3608\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.3037 - val_loss: 0.3646\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.3026 - val_loss: 0.3570\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.3017 - val_loss: 0.3499\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2997 - val_loss: 0.3648\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2984 - val_loss: 0.3485\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2968 - val_loss: 0.3567\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2948 - val_loss: 0.3482\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2941 - val_loss: 0.3506\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2933 - val_loss: 0.3402\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2911 - val_loss: 0.3355\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2893 - val_loss: 0.3408\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2875 - val_loss: 0.3444\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2870 - val_loss: 0.3519\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2875 - val_loss: 0.3337\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2847 - val_loss: 0.3497\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2835 - val_loss: 0.3578\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2826 - val_loss: 0.3392\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2812 - val_loss: 0.3363\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2807 - val_loss: 0.3483\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2793 - val_loss: 0.3295\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2789 - val_loss: 0.3418\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2777 - val_loss: 0.3508\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.2777 - val_loss: 0.3332\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2773 - val_loss: 0.3413\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2754 - val_loss: 0.3303\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2747 - val_loss: 0.3391\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2725 - val_loss: 0.3175\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2733 - val_loss: 0.3709\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2738 - val_loss: 0.3175\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2716 - val_loss: 0.3286\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2708 - val_loss: 0.3251\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2698 - val_loss: 0.3212\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2694 - val_loss: 0.3284\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2679 - val_loss: 0.3237\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.2672 - val_loss: 0.3263\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2669 - val_loss: 0.3415\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2647 - val_loss: 0.3388\n",
      "Epoch 65/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2663 - val_loss: 0.3169\n",
      "Epoch 66/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2642 - val_loss: 0.3466\n",
      "Epoch 67/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2638 - val_loss: 0.3284\n",
      "Epoch 68/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2635 - val_loss: 0.3313\n",
      "Epoch 69/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2620 - val_loss: 0.3123\n",
      "Epoch 70/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.2613 - val_loss: 0.3391\n",
      "Epoch 71/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2607 - val_loss: 0.3112\n",
      "Epoch 72/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: 0.2612 - val_loss: 0.3258\n",
      "Epoch 73/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2593 - val_loss: 0.3204\n",
      "Epoch 74/100\n",
      "7740/7740 [==============================] - 0s 58us/sample - loss: 0.2587 - val_loss: 0.3185\n",
      "Epoch 75/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2583 - val_loss: 0.3197\n",
      "Epoch 76/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2581 - val_loss: 0.3231\n",
      "Epoch 77/100\n",
      "7740/7740 [==============================] - 0s 62us/sample - loss: 0.2594 - val_loss: 0.3199\n",
      "Epoch 78/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2570 - val_loss: 0.3304\n",
      "Epoch 79/100\n",
      "7740/7740 [==============================] - 0s 61us/sample - loss: 0.2559 - val_loss: 0.3304\n",
      "Epoch 80/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2545 - val_loss: 0.3237\n",
      "Epoch 81/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.2544 - val_loss: 0.3429\n",
      "3870/3870 [==============================] - 0s 26us/sample - loss: 0.3022\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 0.2668\n",
      "[CV]  n_neurons=88, n_hidden=3, learning_rate=0.0043533387647423545, total=  39.1s\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 16.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11610/11610 [==============================] - 1s 120us/sample - loss: 0.8786 - val_loss: 0.6314\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4798 - val_loss: 0.5229\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4100 - val_loss: 0.4248\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3806 - val_loss: 0.4156\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3683 - val_loss: 0.3948\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3564 - val_loss: 0.3958\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3545 - val_loss: 0.4230\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3452 - val_loss: 0.3876\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3423 - val_loss: 0.4232\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3371 - val_loss: 0.3704\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3331 - val_loss: 0.3927\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3297 - val_loss: 0.3694\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3263 - val_loss: 0.3754\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3233 - val_loss: 0.3666\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3203 - val_loss: 0.3642\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3171 - val_loss: 0.3664\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3155 - val_loss: 0.3583\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3126 - val_loss: 0.3742\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3098 - val_loss: 0.3521\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3089 - val_loss: 0.3527\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3058 - val_loss: 0.3492\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3041 - val_loss: 0.3407\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3026 - val_loss: 0.3453\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3001 - val_loss: 0.3467\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.2988 - val_loss: 0.3386\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.2974 - val_loss: 0.3532\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.2950 - val_loss: 0.3404\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2935 - val_loss: 0.3375\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.2906 - val_loss: 0.3499\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.2917 - val_loss: 0.3297\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2890 - val_loss: 0.3712\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2890 - val_loss: 0.3285\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.2872 - val_loss: 0.3380\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.2848 - val_loss: 0.3490\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.2834 - val_loss: 0.3203\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.2817 - val_loss: 0.3424\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2815 - val_loss: 0.3218\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.2797 - val_loss: 0.3259\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.2783 - val_loss: 0.3273\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.2770 - val_loss: 0.3206\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.2772 - val_loss: 0.3209\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.2753 - val_loss: 0.3560\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2775 - val_loss: 0.3204\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2729 - val_loss: 0.3307\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.2728 - val_loss: 0.3263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f87fbedf510>,\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_hidden': [0, 1, 2, 3], 'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, ...6591264410276, 0.00561403910405859, 0.015148912343132669, 0.004025069608970383, 0.0101290138640658]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(), \n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es idéntico a cualquier RandomizedSearchCV de Sklearn, con la excepción de que le pasamos parámetros extra que caen directamente al modelo Keras. Además, RandomizedSearchCV utiliza un K-Fold CV, por lo que no usa X_valid ni y_valid, estos solo se usan para el callback de early stopping. \n",
    "\n",
    "Esta ejecución puede durar horas en función del hardware, es un entrenamiento MUY pesado. Una vez acabe, se pueden explorar los parámetros típicos de cualquier ejecución de Sklearn con un RandomizedSearchCV, por ejemplo:\n",
    "\n",
    "    - rnd_search_cv.best_params_\n",
    "    - rnd_search_cv.best_score_\n",
    "    - rnd_search_cv.best_estimator_\n",
    "    - rnd_search_cv.score(X_test, y_test)\n",
    "    - etc.\n",
    "    \n",
    "Y se puede entrenar el mejor modelo para usarlo con el siguiente código:\n",
    "\n",
    "    model = rnd_search_cv.best_estimator_.model\n",
    "    model\n",
    "    \n",
    "Que evalua con:\n",
    "\n",
    "    model.evaluate(X_test, y_test)\n",
    "    \n",
    "Además, este modelo puede ser guardado, cargado y esas cosas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen formas más eficientes de buscar en el espacio de variables por la combinación correcta. La idea siempre es la misma, buscar una región del espacio que parezca buena y después explorar dentro de esta. Este es un grupo de librerias Python que sirven para optimizar hiperparámetros:\n",
    "\n",
    "    - Hyperopt: librería popular de Python para optimizar sobre todos cualquier tipo de espacio de variables complejo, como learning rates o valores discretos como el número de layers.\n",
    "    - Hyperas, kopt o Talos: Optimización de hiperparámetros de Keras (los dos primeros se basan en Hyperopt).\n",
    "    - Scikit-Optimize (skopt): una librería de optimización de uso general. Tiene un método BayesSearchCV que realiza un CV de optimización Bayesiana con una interfaz similar a GridSearchCV.\n",
    "    - Spearmint: librería de optimización Bayesiana.\n",
    "    - Sklearn-Deap: una librería de optimización de hiperparámetros basada en algoritmos evolutivos. Tiene una interfaz similar a GridSearchCV.\n",
    "    - ...\n",
    "    \n",
    "Existen otras compañias que ofrecen servicios de optimización de hiperparámetros. Por ejemplo, Google Cloud ML Engine tiene este servicio. Otras empresas proveen de APIs de optimización como Arimo, SigOpt o Oscar.\n",
    "\n",
    "El tuneo de hiperparámetros es un área activa de investigación, y los algoritmos evolutivos están revolucionando el área. Por ejemplo, DeepMint sacó un paper en 2017 donde optimizaban conjuntamente un grupo de modelos y sus hiperparámetros. Google además usó un algoritmo evolutivo no sólo para encontrar los mejores hiperparámetros, si no también la mejor estructura de NN para el problema. Lo llamaron AutoML, y está disponible como un servicio cloud. (Post sobre esto https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html)\n",
    "\n",
    "Esto abre la puerta al pensamiento de que la época en la que se construyan las arquitecturas de NN de forma manual están llegando a su fin. De hecho los algoritmos evolutivos se están usando de forma eficiente para entrenar NN individuales reemplazando a los Gradient Descent. (Post sobre esto https://eng.uber.com/deep-neuroevolution/).\n",
    "\n",
    "A pesar de todo esto, siempre ayuda tener una idea general de cómo selecionar valores razonables para los hiperparámetros. Eso vamos a ver ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Número de capas ocultas</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para casi todos los problemas se puede empezar con una única capa. Teóricamente un MLP con una capa puede realizar cualquier función compleja si tiene las suficientes neuronas. El motivo por el que se usan deep networks es que son mucho más eficientes en cuanto a parámetros: pueden realizar las mismas tareas complejas con un valor exponencialmente menor de neuronas, lo que hace que tenga un rendimiento muchisimo mejor con el mismo número de datos.\n",
    "\n",
    "La explicación a esto es que los datos suelen estar estructurados de una forma jerárquica, y las redes neuronales profundas se aprovechan de ello. Las capas mas bajas modelan estructuras de bajo nivel (p.e. lineas y segmentos de varias direcciones y formas), las capas intermedias modelan estructuras de nivel medio (por ejemplo cuadrados, circulos...) y las capas altas modelan estructuras complejas (por ejemplo caras). Esto permite converger de una forma más eficiente a soluciones mejores e incluso generalizar mejor para datasets nuevos. Por ejemplo, si tienes un modelo para reconocer caras en fotos, y quieres entrenar uno que reconozca cortes de pelo, puedes reutilizar las capas bajas y reentrenar las altas. Esto se llama Transfer Learning, y acelera enormemente el entrenamiento.\n",
    "\n",
    "Para la mayoría de problemas puedes empezar con una o dos capas ocultas (p.e. el MNIST se modela a un 97% con una capa oculta, y mas del 98% con 2, de forma muy sencilla además). Para problemas complejos, se puede ir aumentando el número de capas ocultas hasta llegar al overfitting. Problemas complejos, como image classification o spech recognition, suelen requerir decenas o centenares de capas (pero normalmente no suelen ser fully-connected, esto ya lo veremos en un notebook posterior). Sin embargo nunca se suelen entrenar de 0, lo normal es reutilizar partes de redes entrenadas en el state-of-the-art que hagan una tarea similar. Esto agiliza muchisimo el entrenamiento y reduce drásticamente la cantidad de datos utilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Número de neuronas por capa</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviamente, las neuronas de las capas input y output vienen determinadas por el tipo de input y output de tu problema (p.e. MNIST son 28x28 input y 10 output).\n",
    "\n",
    "Era típico estructurar las capas ocultas en forma de pirámide. Sin embargo, se ha abandonado esta idea al parecer que usar el mismo número de neuronas para cada capa funciona igual (y a veces mejor), y sólo se debe modelar un hiperparámetro en lugar de uno cada capa. Depende de cada problema, y algunas veces si que compensa hacer la primera capa mayor.\n",
    "\n",
    "Igual que para el número de capas, se puede aumentar el número de neuronas hasta que se llegue al overfitting. Suele ser mas rentable aumentar el número de capas, y saber el número correcto de neuronas es todavía una caja negra.\n",
    "\n",
    "Existen técnicas, como coger un modelo con más neuronas de las que necesitas y usar early stopping para prevenir overfitting (u otras técnicas que ya veremos, como el dropout). Esto se conoce como la técnica de \"strech pants\": en vez de gastar tiempo en buscar los pantalones de tu talla perfecta, coge unos grandes y adáptalos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Learning Rate y otros</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen otros hiperparámetros en las MLP además de los números de capas y de neuronas. Algunos son los siguientes:\n",
    "\n",
    "    - Learning rate: es uno de los hiperparámetros más importantes. El learning rate idóneo suele ser aproximadamente la mitad del learning rate máximo (es decir, el que hace que el modelo diverja). Por esto, una forma buena de encontrar el learning rate correcto es empezar por uno muy alto que haga que el algoritmo diverja, luego dividirlo entre 3 y volver a probar, y así hasta que deje de diverger. En este punto no estarás muy lejos del óptimo. Además, a veces es útil reducir el learning rate durante el entrenamiento (ya lo veremos).\n",
    "    - Elegir un optimizer mejor que el típico Mini-Batch GD y optimizar sus hiperparámetros. Ya lo veremos.\n",
    "        - El batch size a veces tiene mucho impacto en los modelos. En general nunca se deben usar mini-batches mayores que 32, Un small batch pequeño asegura que cada iteración es grande, y aunque en teoría un batch size mayor da una estimación mejor de los gradientes, en la práctica no importa ya que el landscape de optimización es muy complejo y la dirección de los gradientes reales no suelen apuntar a la dirección óptima. Sin embargo, un batch size mayor que 10 suele aprovechar la optimización de hardware y software (sobre todo para multiplicación de matrices) acelerando el entrenamiento.\n",
    "        - Elegir una función de activación correcta, como ya se discutió anteriormente. En general la ReLu funciona bien para las capas ocultas, y para la output depende del problema.\n",
    "        - En la mayoria de los casos, en lugar de definir el número de iteraciones, es mejor usar early stopping.\n",
    "        \n",
    "En los siguientes notebooks veremos técnicas para entrenar redes profundas, veremos cómo personalizar los modelos usando la API de TensorFlow y como preprocesar datos con la Data API. Además veremos distintas arquitecturas de Redes Neuronales, como Redes Neuronales Convolucionales para imágenes, Redes Neuronales Recurrentes para datos secuenciales, Autoencoders para representation learning y GANs (Generative Adversarial Networks) para modelizar y generar datos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
